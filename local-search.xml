<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>马列主义入门书单</title>
    <link href="/2022/05/12/marxism_booklist/"/>
    <url>/2022/05/12/marxism_booklist/</url>
    
    <content type="html"><![CDATA[<h1 id="【转载】马列主义入门小书单"><a href="#【转载】马列主义入门小书单" class="headerlink" title="【转载】马列主义入门小书单"></a>【转载】<a href="https://zhuanlan.zhihu.com/p/375574038">马列主义入门小书单</a></h1><p><a href="https://www.zhihu.com/people/LinkinBryant">林布</a></p><p>写这个是因为之前有人问，然后刚好我在知乎上刷到了一个书单推荐，颇匪夷所思。不过此处不隔空对线了。</p><h2 id="0，基本的方法论"><a href="#0，基本的方法论" class="headerlink" title="0，基本的方法论"></a><strong>0，基本的方法论</strong></h2><p>想要很好地认识马列主义，其实往往并不是直接从哲学性的、思想性的阅读中来的。<br>我个人是在学习历史的过程中开始感受到历史唯物主义和辩证唯物主义的重要性，尤其是其作为方法论、作为在史书之外的角度看待历史的价值。当然，更好的办法是广泛地亲自参与社会实践。</p><p>当然这很漫长，也不适用于所有人。所以我还是要列出一个简单的书单来。对于马列主义经典作者的阅读，必定要秉承着唯物史观的方法论。具体来说就是：经典论述→一般原理→具体现实→具体方法。</p><p>避免教条主义，避免刻舟求剑，但是同样要抓住马列主义最核心的原则。既要避免左灯右转，也要避免原教旨念经。关于马列主义的核心原则，一言蔽之，科学性和革命性。具体不展开说了。我很喜欢列宁援引过的一句话。科学如果不能得到正确的哲学的帮助，就会跌入反动的泥坑。而所谓战斗的唯物主义，（如果脱离了科学的指导），那与其说是战斗，不如说是挨揍。</p><h2 id="1，看书之前，先看一篇论文"><a href="#1，看书之前，先看一篇论文" class="headerlink" title="1，看书之前，先看一篇论文"></a><strong>1，看书之前，先看一篇论文</strong></h2><p>武汉大学赵凯荣教授的《<a href="https://link.zhihu.com/?target=https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CCJD&dbname=CCJDLAST2&filename=MKZY201701013&v=8vFlCYXUNNVHdPRmUVaiVSngpt4AHKXitlaII%25mmd2FxVAYKjf%25mmd2BfRCMrBbS8%25mmd2Fwi4AKdvT">意识形态的实践结构及列宁对马克思主义的最独特贡献</a>》</p><p>为什么要把这篇论文放在书单的第一篇。是因为马列主义的原作真的比较难啃，而且期间的思想问题的思辨是长期延续的。<br>光看书，而得不到一个正确的核心方向的引领，就很容易陷入到迷茫之中。<br>无论是知乎上的左小将，还是历史上的考茨基。他们看的书不可谓不多。他们做的事，不可谓不错。——好吧有些左小将可能真的没看书。</p><p>而为什么要提这篇论文呢，我总结以下三个原因</p><ul><li><p>论述了马克思的唯物主义“区别于从前的一切唯物主义”的根本要素，就是实践，就是人的实践，也就是马克思所说的“感性的人的活动”。“一条铁路，在被制造出来之前，就已经在意识里存在了。然而不被制造出来，就不足以称为意识形态在实践上的完成”<br>“一条铁路，如果仅仅是被制造出来。而不通车、不被使用、不被磨损，那它也不是真正的铁路，仅仅是可能成为铁路的某个象形”</p></li><li><p>提示了从唯物史观角度学习马列主义的方法论。论述了马克思主义与列宁主义的联系和创新。尤其是，如何在不同的客观条件下，根据旧的论述、总结一般性原理，再结合客观条件去建立新的理论。而这根本上是为了理论在实践层面上的完成。<br>尤其是，这同样论述了马列主义作为一个开放的、发展的理论体系的特征。以及马克思主义与列宁主义的整体性。</p></li><li><p>一个真正有理论高度的学者是如何对观点进行论述的。这当然没有固定的范式。但是你如何去看一篇文章，怎么去认识到这个作者的论述是有高度的、是可信的。我觉得这篇论文可以当范本。</p></li></ul><h2 id="2，这回真的是书单了，第一部分"><a href="#2，这回真的是书单了，第一部分" class="headerlink" title="2，这回真的是书单了，第一部分"></a><strong>2，这回真的是书单了，第一部分</strong></h2><h3 id="2-1，列宁《国家与革命》、马克思《哥达纲领批判》。"><a href="#2-1，列宁《国家与革命》、马克思《哥达纲领批判》。" class="headerlink" title="2.1，列宁《国家与革命》、马克思《哥达纲领批判》。"></a>2.1，列宁《国家与革命》、马克思《哥达纲领批判》。</h3><p>这两篇文章论述的都是马列主义的国家观和社会观，其基本概念基本上可以在中学政治书里看到。类似于专政工具、暴力机关等概念。</p><p>把这两篇放在开头，是因为他对入门者友好。而且其主题，也是那些对马列主义感兴趣的人所关心的。</p><p>而马列主义在其中剥开阶级社会文明、道德的面纱，把其阶级本质暴露出来的做法。既是批判，也为马列主义定下了一个基调，即抛开道德批判，关注于阶级批判与经济批判。</p><h3 id="2-2，列宁《论战斗唯物主义的意义》"><a href="#2-2，列宁《论战斗唯物主义的意义》" class="headerlink" title="2.2，列宁《论战斗唯物主义的意义》"></a>2.2，列宁《论战斗唯物主义的意义》</h3><p>这一篇是比较冷门的。推荐的理由就是短。还有就是可读性比较强。<br>因为列宁是个很杠的人，而且他的政论时时刻刻透出一种刻薄尖锐的味道。而文章的主题和针对的问题也大多比较明确。 而我前面提到的那个“与其说是战斗不如说是挨揍”也是出自一篇。<br>对于入门者来说，读来比较好玩。兴趣比经学重要。更何况，这一篇还重点论述了我认为很重要的科学与哲学的问题。</p><h3 id="2-3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言"><a href="#2-3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言" class="headerlink" title="2.3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言"></a>2.3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言</h3><p>资本论第一卷只用粗读。反正你以后还是得回来的。<br>涉及到比较抽象的概念，觉得自己看得眼神涣散的，就简单点略过。重点可以看其中的第三章、第六章，概念性的东西不多。主要是陈述性的东西。</p><p>然后反过来看《德意志意识形态》和《政治经济学批判》序言，主要关注唯物史观的部分。</p><h3 id="2-4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》"><a href="#2-4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》" class="headerlink" title="2.4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》"></a>2.4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》</h3><p>这里提到的两位作者都是国内学界的…老师。他们可能是很多马哲学者老师的老师。<br>实话说，但凡把这两部作品不太囫囵的看完了的话，水平就不知道比我高到哪里去了。</p><p>《马克思主义哲学原理》这部书有很多同名图书，作者都不同。请认准陈先达老师，主要是别的我没看过，不保证质量。</p><p>作为延伸阅读，可以考虑陈先达老师的《马克思主义基础理论若干重大问题研究》</p><h2 id="3，书单的第二部分"><a href="#3，书单的第二部分" class="headerlink" title="3，书单的第二部分"></a><strong>3，书单的第二部分</strong></h2><p>前面的第一部分是解决基本概念、基本思维和基本问题的部分。而第二部分则更侧重于进一步地从历史中去认识马列主义。实际上我自己也还在这个阶段划水，一眼没看到边。只能根据自己的阅读和计划来去重新厘清这些问题。</p><h3 id="3-1，是张一兵教授的《回到马克思》与《回到列宁》"><a href="#3-1，是张一兵教授的《回到马克思》与《回到列宁》" class="headerlink" title="3.1，是张一兵教授的《回到马克思》与《回到列宁》"></a>3.1，是张一兵教授的《回到马克思》与《回到列宁》</h3><p>这两本著作是对马列二人的思想的一个深度梳理，而且能够帮助读者把他们各自的经历、思想脉络串联起来。在这里，插播一条同样是张一兵教授的论文《列宁阅读黑格尔哲学的初始视界》<br>不长，简单一句话：看不懂很正常。张一兵教授的这篇论文说的就是，列宁当时都没看明白，一边疯狂吐槽一边看，看完反思以后继续吐槽。</p><h3 id="3-2，马克思《1844年经济学哲学手稿》、《资本论》第一卷"><a href="#3-2，马克思《1844年经济学哲学手稿》、《资本论》第一卷" class="headerlink" title="3.2，马克思《1844年经济学哲学手稿》、《资本论》第一卷"></a>3.2，马克思《1844年经济学哲学手稿》、《资本论》第一卷</h3><p>资本论第一卷又回来了。这次可以看的细一些，结合张教授的《回到马克思》去重新捋一遍，这次可以重点关注其中的政治经济学内容。</p><h3 id="3-3，恩格斯《反杜林论》"><a href="#3-3，恩格斯《反杜林论》" class="headerlink" title="3.3，恩格斯《反杜林论》"></a>3.3，恩格斯《反杜林论》</h3><p>对于马克思主义你有看不明白的地方，就去看《反杜林论》。</p><p>你和人争辩，觉得对方不对又说不清楚哪里不对的地方，就去看《反杜林论》</p><p>恩格斯yyds！</p><h3 id="3-4-，列宁《唯物主义与经验批判主义》"><a href="#3-4-，列宁《唯物主义与经验批判主义》" class="headerlink" title="3.4 ，列宁《唯物主义与经验批判主义》"></a>3.4 ，列宁《唯物主义与经验批判主义》</h3><p>这是列宁批判唯心主义的大作。也是列宁的哲学思想中最引人争议的地方（指哲学的党性原则）<br>实际上这一篇更重要的地方是对唯物主义认识论的论述。但是关于党性原则的争论似乎是无休止的。</p><p>教员曾经有过论述，自然科学是没有阶级性的。但是对科学的利用和研究是有的。<br>抓住这句话去理解党性原则的问题。</p><h3 id="3-4-，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。"><a href="#3-4-，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。" class="headerlink" title="3.4 ，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。"></a>3.4 ，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。</h3><p>这个没必要看原文，虽然列宁的《怎么办》是一篇极为重要的经典著作。</p><p>但是太长了。没必要专门去看。主要是看列宁是如何论证具体的政治经济现状，如何基于他论证的现状去批评不同的思潮的。</p><p>尤其是，列宁为何批评冒进式的民粹主义。如何批评跪在地上挥拳头的工联主义和经济派。<br>这对于当今的舆论环境来说，有特别的意义。</p><p>扩展阅读：知网上关于列宁新经济政策的一系列研读。</p><h2 id="4，书单，第三部分"><a href="#4，书单，第三部分" class="headerlink" title="4，书单，第三部分"></a><strong>4，书单，第三部分</strong></h2><p>我还没看，但觉得有必要看，又或者看了确实没看明白的书目。</p><p>黑格尔《逻辑学》。列宁《哲学笔记》、马克思《黑格尔法哲学批判》，以及张一兵教授对《哲学笔记》的一系列研读</p><p>列宁说过看不懂得逻辑学就不可能懂得辩证唯物主义。而我可以自豪的宣称，我特么真的不懂。</p><p>然后就是对《资本论》、尤其是对政治经济学内容的进一步研读。尤其关注其中概念性和定义性的问题。<br>另外，对于资本论，把握恩格斯的一个论述：资本论并非是宣扬某种理想国或者乌托邦的作品，而是对现实存在的社会制度的剖析。<br>然后到了这里，再去看别的书就可以比较随意了。有需要有兴趣的就找来看</p><h2 id="5，评论区拾遗"><a href="#5，评论区拾遗" class="headerlink" title="5，评论区拾遗"></a><strong>5，评论区拾遗</strong></h2><p>为什么我这个书单里没有近现代中国的著作?<br>一方面是因为我个人的入门方向是列宁主义，对近现代的中国的东西学习尚未能成体系，难以进行有价值的推荐。另一方面，是由于不是很方便讨论。</p><p>但是这里还是补拾了一些评论区里提到的，值得一读的作品。这里再次感谢所有人给出的意见。</p><ol><li>教员的《实践论》《矛盾论》<br>十分重要而且成体系的著作。一开始可以粗读，然后可以放在第二部分再次精读细读。</li><li>恩格斯的《自然辩证法》<br>评论区的知友指出这是适合入门的作品，可以自行取舍。</li><li>徐禾的《政治经济学概论》,李达的《唯物辩证法大纲》<br>国内非常有高度的经典教材。综合性和概念性比较强，有强烈兴趣的朋友可以考虑参考这两本书进行系统性阅读。如果觉得看着吃力，则可以考虑先跳过，遇到问题以后再回来对照着看。</li><li>教员的《中国社会各阶级分析》，《论持久战》，《中国革命战争的战略问题》<br>我估计只要提到了，后面就不用我多说什么了。结合具体的时代背景去了解这几篇。</li><li>恩格斯的《家庭、私有制和国家起源》、列宁的《论国家》<br>作为《哥达纲领批判》《国家与革命》的延伸阅读。另外我个人是十分喜欢恩格斯的文章的。只是实在是最近在划水，从外在需求上来说实在是没能系统性地去看恩格斯的东西。《家庭、私有制和国家起源》作为入门作品而言略有些嫌长了，但是也可以先看一、二、九章。恩格斯在这本书里的论述是唯物史观的集中体现，可以说，唯物史观到底是怎么个东西。读完恩格斯的这一篇，不用别人给你定义，自身也能有个概念了。</li><li>《共产党宣言》<br>有人问《共产党宣言》这么具有代表性的作品为什么不放进来。我在评论区和两位知友探讨了很多。我觉得《宣言》最好是放在后面看。<br>是从马列主义中去理解《宣言》，而不是从《宣言》中去理解马列主义。</li><li>艾思奇的《大众哲学》和《历史唯物主义和辩证唯物主义》<br>国内比较早的经典作品了。不过我没有看过。评论区有知友认为比较适合入门。作为参考</li><li>《政治的逻辑-马克思主义政治学原理》中国近三十年的意识形态领域的问题。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seq2SeqWithAttention</title>
    <link href="/2022/05/07/Seq2SeqWithAttention/"/>
    <url>/2022/05/07/Seq2SeqWithAttention/</url>
    
    <content type="html"><![CDATA[<h1 id="【转载】Visualizing-A-Neural-Machine-Translation-Model-Mechanics-of-Seq2seq-Models-With-Attention"><a href="#【转载】Visualizing-A-Neural-Machine-Translation-Model-Mechanics-of-Seq2seq-Models-With-Attention" class="headerlink" title="【转载】Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)"></a>【转载】<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></h1><p>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">using</a> such a model in production in late 2016. These models are explained in the two pioneering papers (<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al., 2014</a>, <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho et al., 2014</a>).</p><p>I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That’s what I aim to do in this post. You’ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post).</p><p>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items. A trained model would work like this:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_1.gif" width heigh></center><p>In neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_2.gif" width heigh></center><h2 id="Looking-under-the-hood"><a href="#Looking-under-the-hood" class="headerlink" title="Looking under the hood"></a>Looking under the hood</h2><p>Under the hood, the model is composed of an encoder and a decoder.</p><p>The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_3.gif" width heigh></center><p>The same applies in the case of machine translation.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_4.gif" width heigh></center><p>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks (Be sure to check out Luis Serrano’s <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">A friendly introduction to Recurrent Neural Networks</a> for an intro to RNNs).</p><p><img src="/2022/05/07/Seq2SeqWithAttention/context.png" alt="context"></p><p>[^ ]: The context is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values.</p><p>You can set the size of the context vector when you set up your model. It is basically the number of hidden units in the <font color="red">encoder</font> $ RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024.</p><p>By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “<a href="https://machinelearningmastery.com/what-are-word-embeddings/">word embedding</a>” algorithms. These turn words into vector spaces that capture a lot of the meaning&#x2F;semantic information of the words (e.g. <a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html">king - man + woman &#x3D; queen</a>).</p><p><img src="/2022/05/07/Seq2SeqWithAttention/embedding.png"></p><p>[^ ]: We need to turn the input words into vectors before processing them. That transformation is done using a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> algorithm. We can use <a href="http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/">pre-trained embeddings</a> or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, we’re showing a vector of size four for simplicity.</p><p>Now that we’ve introduced our main vectors&#x2F;tensors, let’s recap the mechanics of an RNN and establish a visual language to describe these models:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/RNN_1.gif" width heigh></center><p>The next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, we’ll use an animation like this to describe the vectors inside a neural machine translation model.</p><p>In the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.</p><p>Let’s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder.<img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_5.gif"></p><p>The decoder also maintains a hidden state that it passes from one time step to the next. We just didn’t visualize it in this graphic because we’re concerned with the major parts of the model for now.</p><p>Let’s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an “unrolled” view where instead of showing the one decoder, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_6.gif" width heigh></center><h1 id="Let’s-Pay-Attention-Now"><a href="#Let’s-Pay-Attention-Now" class="headerlink" title="Let’s Pay Attention Now"></a>Let’s Pay Attention Now</h1><p>The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2014</a> and <a href="https://arxiv.org/abs/1508.04025">Luong et al., 2015</a>. These papers introduced and refined a technique called “Attention”, which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed.</p><p><img src="/2022/05/07/Seq2SeqWithAttention/attention.png" alt="attention"></p><p>[^ ]: At time step 7, the attention mechanism enables the decoder to focus on the word “étudiant” (“student” in french) before it generates the English translation. This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention.</p><p>Let’s continue looking at attention models at this high level of abstraction. An attention model differs from a classic sequence-to-sequence model in two main ways:</p><p>First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes <em>all</em> the hidden states to the decoder:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_7.gif" width heigh></center><p>Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:</p><ol><li>Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence</li><li>Give each hidden state a score (let’s ignore how the scoring is done for now)</li><li>Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores</li></ol><center><img src="/2022/05/07/Seq2SeqWithAttention/attention_process.gif" width heigh></center><p>This scoring exercise is done at each time step on the decoder side.</p><p>Let us now bring the whole thing together in the following visualization and look at how the attention process works:</p><ol><li>The attention decoder RNN takes in the embedding of the <END> token, and an initial decoder hidden state.</END></li><li>The RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded.</li><li>Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.</li><li>We concatenate h4 and C4 into one vector.</li><li>We pass this vector through a feedforward neural network (one trained jointly with the model).</li><li>The output of the feedforward neural networks indicates the output word of this time step.</li><li>Repeat for the next time steps</li></ol><center><img src="/2022/05/07/Seq2SeqWithAttention/attention_tensor_dance.gif" width heigh></center><p>This is another way to look at which part of the input sentence we’re paying attention to at each decoding step:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_9.gif"></center><p>Note that the model isn’t just mindless aligning the first word at the output with the first word from the input. It actually learned from the training phase how to align words in that language pair (French and English in our example). An example for how precise this mechanism can be comes from the attention papers listed above:</p><p><img src="/2022/05/07/Seq2SeqWithAttention/attention_sentence.png"></p><p>[^ ]: You can see how the model paid attention correctly when outputing “European Economic Area”. In French, the order of these words is reversed (“européenne économique zone”) as compared to English. Every other word in the sentence is in similar order.</p><p>If you feel you’re ready to learn the implementation, be sure to check TensorFlow’s <a href="https://github.com/tensorflow/nmt">Neural Machine Translation (seq2seq) Tutorial</a>.</p>]]></content>
    
    
    <categories>
      
      <category>注意力机制</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention</tag>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My first post</title>
    <link href="/2022/04/28/my-first-post/"/>
    <url>/2022/04/28/my-first-post/</url>
    
    <content type="html"><![CDATA[<p>2022&#x2F;4&#x2F;28 首篇博客。<br>年级大了，怕一些事情一些学过的知识记不住，开始学习用网站记录些什么了。<br>也希望这记录的点点滴滴能督促我更充实地活！</p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>练习</tag>
      
      <tag>开篇</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/04/28/hello-world/"/>
    <url>/2022/04/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
