<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>漫谈图神经网络模型一</title>
    <link href="/2022/05/17/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/"/>
    <url>/2022/05/17/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/</url>
    
    <content type="html"><![CDATA[<h1 id="【转载】从图-Graph-到图卷积-Graph-Convolution-：漫谈图神经网络模型-一"><a href="#【转载】从图-Graph-到图卷积-Graph-Convolution-：漫谈图神经网络模型-一" class="headerlink" title="【转载】从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (一)"></a><a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html">【转载】从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (一)</a></h1><p>本文属于图神经网络的系列文章，文章目录如下：</p><ul><li><a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html"><strong>从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (一)</strong></a></li><li><a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html">从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (二)</a></li><li><a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_3.html">从图(Graph)到图卷积(Graph Convolution)：漫谈图神经网络模型 (三)</a></li></ul><hr><p>笔者最近看了一些图与图卷积神经网络的论文，深感其强大，但一些Survey或教程默认了读者对图神经网络背景知识的了解，对未学过信号处理的读者不太友好。同时，很多教程只讲是什么，不讲为什么，也没有梳理清楚不同网络结构的区别与设计初衷(Motivation)。</p><p>因此，本文试图沿着图神经网络的历史脉络，从最早基于不动点理论的<strong>图神经网络</strong>(Graph Neural Network， GNN)一步步讲到当前用得最火的<strong>图卷积神经网络</strong>(Graph Convolutional Neural Network， GCN)， 期望通过本文带给读者一些灵感与启示。</p><ul><li>本文的提纲与叙述要点主要参考了3篇图神经网络的Survey，分别是来自IEEE Fellow的<em>A Comprehensive Survey on Graph Neural Networks</em>[1] 来自清华大学朱文武老师组的<em>Deep Learning on Graphs: A Survey</em>[7]，以及来自清华大学孙茂松老师组的<em>Graph Neural Networks: A Review of Methods and Applications</em>[14]，在这里向三篇Survey的作者表示敬意。</li><li>同时，本文关于部分图卷积神经网络的<strong>理解</strong>很多都是受到知乎问题[8]高赞答案的启发，非常感谢他们的无私分享！</li><li>最后，本文还引用了一些来自互联网的生动形象的图片，在这里也向这些图片的作者表示感谢。本文中未注明出处的图片均为笔者制作，如需转载或引用请联系本人。</li></ul><h2 id="历史脉络"><a href="#历史脉络" class="headerlink" title="历史脉络#"></a>历史脉络<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%8E%86%E5%8F%B2%E8%84%89%E7%BB%9C">#</a></h2><p>在开始正文之前，笔者先带大家回顾一下图神经网络的发展历史。不过，因为图神经网络的发展分支非常之多，笔者某些叙述可能并不全面，一家之言仅供各位读者参考：</p><ol><li>图神经网络的概念最早在2005年提出。2009年Franco博士在其论文 [2]中定义了图神经网络的理论基础，笔者呆会要讲的第一种图神经网络也是基于这篇论文。</li><li>最早的GNN主要解决的还是如分子结构分类等严格意义上的图论问题。但实际上欧式空间(比如像图像 Image)或者是序列(比如像文本 Text)，许多常见场景也都可以转换成图(Graph)，然后就能使用图神经网络技术来建模。</li><li>2009年后图神经网络也陆续有一些相关研究，但没有太大波澜。直到2013年，在图信号处理(Graph Signal Processing)的基础上，Bruna(这位是LeCun的学生)在文献 [3]中首次提出图上的基于频域(Spectral-domain)和基于空域(Spatial-domain)的卷积神经网络。</li><li>其后至今，学界提出了很多基于空域的图卷积方式，也有不少学者试图通过统一的框架将前人的工作统一起来。而基于频域的工作相对较少，只受到部分学者的青睐。</li><li>值得一提的是，图神经网络与图表示学习(Represent Learning for Graph)的发展历程也惊人地相似。2014年，在word2vec [4]的启发下，Perozzi等人提出了DeepWalk [5]，开启了深度学习时代图表示学习的大门。更有趣的是，就在几乎一样的时间，Bordes等人提出了大名鼎鼎的TransE [6]，为知识图谱的分布式表示(Represent Learning for Knowledge Graph)奠定了基础。</li></ol><h2 id="图神经网络-Graph-Neural-Network"><a href="#图神经网络-Graph-Neural-Network" class="headerlink" title="图神经网络(Graph Neural Network)#"></a>图神经网络(Graph Neural Network)<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cgraph-neural-network">#</a></h2><p>首先要澄清一点，除非特别指明，本文中所提到的图均指<strong>图论中的图</strong>(Graph)。它是一种由若干个<strong>结点</strong>(Node)及连接两个结点的<strong>边</strong>(Edge)所构成的图形，用于刻画不同结点之间的关系。下面是一个生动的例子，图片来自论文[14]:</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-1-image-and-graph.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-1-image-and-graph.png" alt="图像与图示例"></a></p><h3 id="状态更新与输出"><a href="#状态更新与输出" class="headerlink" title="状态更新与输出#"></a>状态更新与输出<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E7%8A%B6%E6%80%81%E6%9B%B4%E6%96%B0%E4%B8%8E%E8%BE%93%E5%87%BA">#</a></h3><p>最早的图神经网络起源于Franco博士的论文[2], 它的理论基础是<strong>不动点</strong>理论。给定一张图$\pmb G$，每个结点都有其自己的特征(feature), 本文中用$\pmb {x_v}$表示结点v的特征；连接两个结点的边也有自己的特征，本文中用表示结点$\pmb v$与结点$\pmb u$之间边的特征；GNN的学习目标是获得每个结点的图感知的隐藏状态 $\pmb {h_v}$(state embedding)，这就意味着：对于每个节点，它的隐藏状态包含了来自邻居节点的信息。那么，如何让每个结点都感知到图上其他的结点呢？GNN通过<strong>迭代式更新</strong>所有结点的隐藏状态来实现，在$\pmb {t+1}$时刻，结点$\pmb v$的隐藏状态按照如下方式更新：<br>$$<br>\pmb{h}_v^{t+1}&#x3D;f(\pmb{x}_v,\pmb{x}_c\pmb{o}[v],\pmb{h}_n^te[v],\pmb{x}<em>ne[v])<br>$$<br>上面这个公式中的$f$就是隐藏状态的<strong>状态更新</strong>函数，在论文中也被称为<strong>局部转移函数</strong>(local transaction function)。公式中的$\pmb {x_co[v]}$指的是与结点$\pmb v$相邻的边的特征，$\pmb {x</em>{(v,u)}}$指的是结点$\pmb v$的邻居结点的特征，$\pmb {h_v}$则指邻居结点在$\pmb t$时刻的隐藏状态。注意 $\pmb f$ 是对所有结点都成立的，是一个全局共享的函数。那么怎么把它跟深度学习结合在一起呢？聪明的读者应该想到了，那就是利用神经网络(Neural Network)来拟合这个复杂函数 $\pmb f$。值得一提的是，虽然看起来$\pmb f$ 的输入是不定长参数，但在 $\pmb f$ 内部我们可以先将不定长的参数通过一定操作变成一个固定的参数，比如说用所有隐藏状态的加和来代表所有隐藏状态。我们举个例子来说明一下：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-2-state-update-function.png"><img src="/2022/05/17/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-2-state-update-function.png" alt="更新公式示例" style="zoom: 67%;"></a></p><p>假设结点$\pmb 5$为中心结点，其隐藏状态的更新函数如图所示。这个更新公式表达的思想自然又贴切：不断地利用当前时刻邻居结点的隐藏状态作为部分输入来生成下一时刻中心结点的隐藏状态，直到每个结点的隐藏状态变化幅度很小，整个图的信息流动趋于平稳。至此，每个结点都“知晓”了其邻居的信息。状态更新公式仅描述了如何获取每个结点的隐藏状态，除它以外，我们还需要另外一个函数 $\pmb g$ 来描述如何适应下游任务。举个例子，给定一个社交网络，一个可能的下游任务是判断各个结点是否为水军账号。<br>$$<br>\pmb{o}_v&#x3D;g(\pmb{h}_v,\pmb{x}_v)<br>$$<br>在原论文中，$\pmb g$又被称为<strong>局部输出函数</strong>(local output function)，与 $\pmb f$ 类似，$\pmb g$ 也可以由一个神经网络来表达，它也是一个全局共享的函数。那么，整个流程可以用下面这张图表达：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-4-state-flow.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-4-state-flow.png" alt="更新公式示例"></a></p><p>仔细观察两个时刻之间的连线，它与图的连线密切相关。比如说在$\pmb {T_1}$ 时刻，结点 1 的状态接受来自结点 3 的上一时刻的隐藏状态，因为结点 1 与结点 3相邻。直到 $\pmb {T_n}$时刻，各个结点隐藏状态收敛，每个结点后面接一个 $\pmb g$ 即可得到该结点的输出 $\pmb o$。</p><p>对于不同的图来说，收敛的时刻可能不同，因为收敛是通过两个时刻$\pmb {p-}$范数的差值是否小于某个阈值 $\pmb ϵ$来判定的，比如：<br>$$<br>||\pmb{H}^{t+1}||^2−||\pmb{H}^{t}||^2&lt;ϵ<br>$$</p><h3 id="实例-化合物分类"><a href="#实例-化合物分类" class="headerlink" title="实例:化合物分类#"></a>实例:化合物分类<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%AE%9E%E4%BE%8B%E5%8C%96%E5%90%88%E7%89%A9%E5%88%86%E7%B1%BB">#</a></h3><p>下面让我们举个实例来说明图神经网络是如何应用在实际场景中的，这个例子来源于论文[2]。假设我们现在有这样一个任务，给定一个环烃化合物的分子结构(包括原子类型，原子键等)，模型学习的目标是判断其是否有害。这是一个典型的二分类问题，一个训练样本如下图所示：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-6-gnn-example.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-6-gnn-example.png" alt="化合物分子结构"></a></p><p>由于化合物的分类实际上需要对整个图进行分类，在论文中，作者将化合物的<strong>根结点</strong>的表示作为整个图的表示，如图上红色的结点所示。Atom feature 中包括了每个原子的类型(Oxygen, 氧原子)、原子自身的属性(Atom Properties)、化合物的一些特征(Global Properties)等。把每个原子看作图中的结点，原子键视作边，一个分子(Molecule)就可以看作一张图。在不断迭代得到根结点氧原子收敛的隐藏状态后，在上面接一个前馈神经网络作为输出层(即$\pmb g$函数)，就可以对整个化合物进行二分类了。</p><blockquote><p>当然，在同构图上根据策略选择同一个根结点对结果也非常重要。但在这里我们不关注这部分细节，感兴趣的读者可以阅读原文。</p></blockquote><h3 id="不动点理论"><a href="#不动点理论" class="headerlink" title="不动点理论#"></a>不动点理论<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E4%B8%8D%E5%8A%A8%E7%82%B9%E7%90%86%E8%AE%BA">#</a></h3><p>在本节的开头我们就提到了，GNN的理论基础是<strong>不动点</strong>(the fixed point)理论，这里的不动点理论专指**<a href="https://zhuanlan.zhihu.com/p/112642861">巴拿赫不动点定理</a><strong>(Banach’s Fixed Point Theorem)。首先我们用 $\pmb F$表示若干个 $\pmb f$ 堆叠得到的一个函数，也称为</strong>全局更新<strong>函数，那么图上所有结点的状态更新公式可以写成：<br>$$<br>\pmb{H}^{t+1}&#x3D;F(\pmb{H}^{t},\pmb{X})<br>$$<br>不动点定理指的就是，不论$\pmb {H^0}$是什么，只要 $\pmb F$ 是个</strong>压缩映射**(contraction map)，$\pmb {H^0}$经过不断迭代都会收敛到某一个固定的点，我们称之为不动点。那压缩映射又是什么呢，一张图可以解释得明明白白：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-3-contraction-map.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-3-contraction-map.png" alt="更新公式示例"></a></p><p>上图的实线箭头就是指映射 $\pmb F$, 任意两个点$\pmb {x,y}$ 在经过 $\pmb F$ 这个映射后，分别变成了 $\pmb {F(x)},\pmb {F(y)}$。压缩映射就是指，$\pmb {d(F(x),F(y))\le cd(x,y),0\le c\textless 1}$。也就是说，经过 $\pmb F$ 变换后的新空间一定比原先的空间要小，原先的空间被压缩了。想象这种压缩的过程不断进行，最终就会把原空间中的所有点映射到一个点上。</p><p>那么肯定会有读者心存疑问，既然$\pmb f$是由神经网络实现的，我们该如何实现它才能保证它是一个压缩映射呢？我们下面来谈谈 $\pmb f$ 的具体实现。</p><h3 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现#"></a>具体实现<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%85%B7%E4%BD%93%E5%AE%9E%E7%8E%B0">#</a></h3><p>在具体实现中， $\pmb f $ 其实通过一个简单的<strong>前馈神经网络</strong>(Feed-forward Neural Network)即可实现。比如说，一种实现方法可以是把每个邻居结点的特征、隐藏状态、每条相连边的特征以及结点本身的特征简单拼接在一起，在经过前馈神经网络后做一次简单的加和。<br>$$<br>\begin{aligned}<br>\pmb{h}_v^{t+1}&amp;&#x3D;f(\pmb{x}_v,\pmb{x}_c\pmb{o}[v],\pmb{h}_v^{t}e[v],\pmb{x}<em>ne[v])<br>\&amp;&#x3D;\sum</em>{u\in ne[v]}FNN([\pmb{x}<em>v;\pmb{x}</em>{(u,v)};\pmb{h}_u^{t};\pmb{x}_u])<br>\end{aligned}<br>$$<br>那我们如何保证 $\pmb f$ 是个压缩映射呢，其实是通过限制 $\pmb f$ 对 $\pmb H$ 的偏导数矩阵的大小，这是通过一个对<strong>雅可比矩阵</strong>(Jacobian Matrix)的<strong>惩罚项</strong>(Penalty)来实现的。在代数中，有一个定理是: $\pmb f$ 为压缩映射的等价条件是 $\pmb f$ 的梯度&#x2F;导数要小于1。这个等价定理可以从压缩映射的形式化定义导出，我们这里使用 $||\pmb x||$表示 $\pmb x$ 在空间中的<strong>范数</strong>(norm)。范数是一个标量，它是向量的长度或者模，$||\pmb x||$ 是 $\pmb x$ 在有限空间中坐标的连续函数。这里把 $\pmb x$ 简化成1维的，坐标之间的差值可以看作向量在空间中的距离，根据压缩映射的定义，可以导出：<br>$$<br>\begin{aligned}<br>||F(x)-F(y)||\leq c||x-y||,0\leq c\textless 1\<br>\frac{||F(x)-F(y)||}{||x-y||}\le c\<br>\frac{||F(x)-F(x-\Delta x)||}{||\Delta x||}\le c\<br>||F’(x)||\le||\frac{\partial F(x)}{\partial x}||\le c<br>\end{aligned}<br>$$<br>推广一下，即得到雅可比矩阵的罚项需要满足其范数小于等于$\pmb c$等价于压缩映射的条件。根据拉格朗日乘子法，将有约束问题变成带罚项的无约束优化问题，训练的目标可表示成如下形式：<br>$$<br>J&#x3D;Loss+\lambda \cdot max(\frac{||\partial FNN||}{\partial h}-c,0),c\in (0,1)<br>$$<br>其中λλ是超参数，与其相乘的项即为雅可比矩阵的罚项。</p><h3 id="模型学习"><a href="#模型学习" class="headerlink" title="模型学习#"></a>模型学习<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E6%A8%A1%E5%9E%8B%E5%AD%A6%E4%B9%A0">#</a></h3><p>上面我们花一定的篇幅搞懂了如何让 $\pmb f$ 接近压缩映射，下面我们来具体叙述一下图神经网络中的损失 $\pmb {Loss}$ 是如何定义，以及模型是如何学习的。</p><p>仍然以社交网络举例，虽然每个结点都会有隐藏状态以及输出，但并不是每个结点都会有<strong>监督信号</strong>(Supervision)。比如说，社交网络中只有部分用户被明确标记了是否为水军账号，这就构成了一个典型的结点二分类问题。</p><p>那么很自然地，模型的损失即通过这些有监督信号的结点得到。假设监督结点一共有 $\pmb p$ 个，模型损失可以形式化为：<br>$$<br>Loss&#x3D;\sum_{i&#x3D;1}^p(t_i-\pmb o_i)<br>$$</p><p>那么，模型如何学习呢？根据<strong>前向传播计算损失</strong>的过程，不难推出<strong>反向传播计算梯度</strong>的过程。在前向传播中，模型：</p><ol><li>调用 $\pmb f$ 若干次，比如 $\pmb {T_n}$次，直到 $\pmb {h_v^{T_n}}$ 收敛。</li><li>此时每个结点的隐藏状态接近不动点的解。</li><li>对于有监督信号的结点，将其隐藏状态通过 $\pmb g$ 得到输出，进而算出模型的损失。</li></ol><p>根据上面的过程，在反向传播时，我们可以直接求出 $\pmb f$ 和 $\pmb g$ 对最终的隐藏状态 $\pmb {h_v^{T_n}}$ 的梯度。然而，因为模型递归调用了 $\pmb f$ 若干次，为计算 $\pmb f$ 和 $\pmb g$ 对最初的隐藏状态 $\pmb {h_v^0}$ 的梯度，我们需要同样递归式&#x2F;迭代式地计算 $\pmb {T_n}$ 次梯度。最终得到的梯度即为 $\pmb f$ 和 $\pmb g$ 对 $\pmb {h_v^0}$ 的梯度，然后该梯度用于更新模型的参数。这个算法就是 Almeida-Pineda 算法[9]。</p><h3 id="GNN与RNN"><a href="#GNN与RNN" class="headerlink" title="GNN与RNN#"></a>GNN与RNN<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#gnn%E4%B8%8Ernn">#</a></h3><p>相信熟悉 RNN&#x2F;LSTM&#x2F;GRU 等循环神经网络的同学看到这里会有一点小困惑，因为图神经网络不论是前向传播的方式，还是反向传播的优化算法，与循环神经网络都有点相像。这并不是你的错觉，实际上，图神经网络与到循环神经网络确实很相似。为了清楚地显示出它们之间的不同，我们用一张图片来解释这两者设计上的不同：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-5-gnn-rnn.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-5-gnn-rnn.png" alt="GNN与RNN的区别"></a></p><p>假设在GNN中存在三个结点$\pmb {x_1}$,$\pmb {x_2}$,$\pmb {x_3}$，相应地，在RNN中有一个序列$\pmb {(x_1,x_2,x_3)}$。笔者认为，GNN与RNN的区别主要在于4点：</p><ul><li>GNN的基础理论是不动点理论，这就意味着GNN沿时间展开的长度是动态的，是<strong>根据收敛条件</strong>确定的，而RNN沿时间展开的长度就等于<strong>序列本身的长度</strong>。</li><li>GNN每次时间步的输入都是所有结点 $\pmb v$ 的特征，而RNN每次时间步的输入是该时刻对应的输入。同时，时间步之间的信息流也不相同，前者由边决定，后者则由序列的读入顺序决定。</li><li>GNN采用 AP 算法反向传播优化，而RNN使用<strong>BPTT</strong>(Back Propogation Through Time)优化。前者对收敛性有要求，而后者对收敛性是没有要求的。</li><li>GNN循环调用 $\pmb f$ 的目标是得到每个结点稳定的隐藏状态，所以只有在隐藏状态收敛后才能输出；而RNN的每个时间步上都可以输出，比如语言模型。</li></ul><p>不过鉴于初代GNN与RNN结构上的相似性，一些文章中也喜欢把它称之为 Recurrent-based GNN，也有一些文章会把它归纳到 Recurrent-based GCN中。之后读者在了解 GCN后会理解为什么人们要如此命名。</p><h3 id="GNN的局限"><a href="#GNN的局限" class="headerlink" title="GNN的局限#"></a>GNN的局限<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#gnn%E7%9A%84%E5%B1%80%E9%99%90">#</a></h3><p>初代GNN，也就是基于循环结构的图神经网络的核心是不动点理论。它的核心观点是<strong>通过结点信息的传播使整张图达到收敛，在其基础上再进行预测</strong>。收敛作为GNN的内核，同样局限了其更广泛的使用，其中最突出的是两个问题：</p><ul><li>GNN只将边作为一种传播手段，但并未区分不同边的功能。虽然我们可以在特征构造阶段($\pmb {x(u,v)}$)为不同类型的边赋予不同的特征，但相比于其他输入，边对结点隐藏状态的影响实在有限。并且GNN没有为边设置独立的可学习参数，也就意味着无法通过模型学习到边的某些特性。</li><li>如果把GNN应用在<em>图表示</em>的场景中，使用不动点理论并不合适。这主要是因为基于不动点的收敛会导致结点之间的隐藏状态间存在较多信息共享，从而导致结点的状态太<strong>过光滑</strong>(Over Smooth)，并且属于结点自身的特征<strong>信息匮乏</strong>(Less Informative)。</li></ul><p>下面这张来自维基百科[13]的图可以形象地解释什么是 Over Smooth，其中我们把整个布局视作一张图，每个像素点与其上下左右以及斜上下左右8个像素点相邻，这决定了信息在图上的流动路径。初始时，蓝色表示没有信息量，如果用向量的概念表达即为空向量；绿色，黄色与红色各自有一部分信息量，表达为非空的特征向量。在图上，信息主要从三块有明显特征的区域向其邻接的像素点流动。一开始<strong>不同像素点的区分非常明显</strong>，但在向不动点过渡的过程中，所有像素点都取向一致，最终整个系统形成均匀分布。这样，虽然每个像素点都感知到了全局的信息，<strong>但我们无法根据它们最终的隐藏状态区分它们</strong>。比如说，根据最终的状态，我们是无法得知哪些像素点最开始时在绿色区域。</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-9-over-smooth.gif"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-9-over-smooth.gif" alt="OverSmooth"></a></p><p>在这里笔者再多说几句。事实上，上面这个图与GNN中的信息流动并不完全等价。从笔者来看，如果我们用物理模型来描述它，上面这个图代表的是初始时有3个热源在散发热量，而后就让它们自由演化；但实际上，GNN在每个时间步都会将结点的特征作为输入来更新隐藏状态，这就好像是放置了若干个永远不灭的热源，热源之间会有互相干扰，但最终不会完全一致。</p><h2 id="门控图神经网络-Gated-Graph-Neural-Network"><a href="#门控图神经网络-Gated-Graph-Neural-Network" class="headerlink" title="门控图神经网络(Gated Graph Neural Network)#"></a>门控图神经网络(Gated Graph Neural Network)<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E9%97%A8%E6%8E%A7%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9Cgated-graph-neural-network">#</a></h2><p>我们上面细致比较了GNN与RNN，可以发现它们有诸多相通之处。那么，我们能不能直接用类似RNN的方法来定义GNN呢? 于是乎，<strong>门控图神经网络</strong>(Gated Graph Neural Network, GGNN) [10]就出现了。虽然在这里它们看起来类似，但实际上，它们的区别非常大，其中最核心的不同即是<strong>门控神经网络不以不动点理论为基础</strong>。这意味着：$\pmb f$ 不再需要是一个压缩映射；迭代不需要到收敛才能输出，可以迭代固定步长；优化算法也从 AP 算法转向 BPTT。</p><h3 id="状态更新"><a href="#状态更新" class="headerlink" title="状态更新#"></a>状态更新<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E7%8A%B6%E6%80%81%E6%9B%B4%E6%96%B0">#</a></h3><p>与图神经网络定义的范式一致，GGNN也有两个过程：状态更新与输出。相比GNN而言，它主要的区别来源于状态更新阶段。具体地，GGNN参考了GRU的设计，把邻居结点的信息视作输入，结点本身的状态视作隐藏状态，其状态更新函数如下:<br>$$<br>h_v^{t+1}&#x3D;GRU(h_v^{t},\sum_{u\in ne[v]}\pmb W_{edge}h_v^t)<br>$$<br>如果读者对GRU的更新公式熟悉的话，对上式应该很好理解。仔细观察上面这个公式，除了GRU式的设计外，GGNN还针对不同类型的边引入了可学习的参数$\pmb W$。每一种 $\pmb {edge}$ 对应一个 $\pmb {W_{edge}}$，这样它就可以处理异构图。</p><p>但是，仔细对比GNN的GGNN的状态更新公式，细心的读者可能会发现：在GNN里需要作为输入的结点特征 $\pmb {x_v}$ 没有出现在GGNN的公式中! 但实际上，这些结点特征对我们的预测至关重要，因为它才是各个结点的根本所在。</p><p>为了处理这个问题，GGNN将结点特征作为隐藏状态初始化的一部分。那么重新回顾一下GGNN的流程，其实就是这样：</p><ul><li>用结点特征初始化各个结点的(部分)隐藏状态。</li><li>对整张图，按照上述状态更新公式固定迭代若干步。</li><li>对部分有监督信号的结点求得模型损失，利用BPTT算法反向传播求得$\pmb {W_{edge}}$和GRU参数的梯度。</li></ul><h3 id="实例1-到达判断"><a href="#实例1-到达判断" class="headerlink" title="实例1:到达判断#"></a>实例1:到达判断<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%AE%9E%E4%BE%8B1%E5%88%B0%E8%BE%BE%E5%88%A4%E6%96%AD">#</a></h3><p>为了便于理解，我们举个来自论文[10]的例子。比如说给定一张图$\pmb g$，开始结点 $\pmb S$，对于任意一个结点 EE，模型判断开始结点是否可以通过图游走至该结点。同样地，这也可以转换成一个对结点的二分类问题，即<code>可以到达</code>和<code>不能到达</code>。下图即描述了这样的过程：</p><p><a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-7-ggnn.png"><img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-7-ggnn.png" alt="$\pmb g$NN实例"></a></p><p>图中的红色结点即开始结点$\pmb S$，绿色结点是我们希望判断的结点$\pmb E$，我们这里称其为结束结点。那么相比于其他结点，这两个结点具有一定特殊性。那我们就可以使用第1维为1来表示开始结点，第2维为1来表示结束结点。最后在对结束结点分类时，如果其隐藏状态的第1维被赋予得到了一个非0的实数值，那意味着它可以到达。</p><p>从初始化的流程我们也可以看出GNN与GGNN的区别：GNN依赖于不动点理论，所以每个结点的隐藏状态即使使用<strong>随机初始化都会收敛到不动点</strong>；GGNN则不同，不同的初始化对GGNN最终的结果影响很大。</p><h3 id="实例2-语义解析"><a href="#实例2-语义解析" class="headerlink" title="实例2:语义解析#"></a>实例2:语义解析<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%AE%9E%E4%BE%8B2%E8%AF%AD%E4%B9%89%E8%A7%A3%E6%9E%90">#</a></h3><p>上面这个例子非常简单形象地说明了GNN与GGNN的不同，由于笔者比较关注Semantic Parsing(语义解析)相关的工作，所以下面我们借用ACL 2019的一篇论文[11]来讲一下GGNN在实际中如何使用，以及它适用于怎样的场景。</p><p>首先为不了解语义解析的读者科普一下，语义解析的主要任务是将自然语言转换成机器语言，在这里笔者特指的是SQL(结构化查询语言，Structured Query Language)，它就是大家所熟知的数据库查询语言。这个任务有什么用呢？它可以让小白用户也能从数据库中获得自己关心的数据。正是因为有了语义解析，用户不再需要学习SQL语言的语法，也不需要有编程基础，可以直接通过自然语言来查询数据库。事实上，语义解析放到今天仍然是一个非常难的任务。除去自然语言与程序语言在语义表达上的差距外，很大一部分性能上的损失是因为任务本身，或者叫SQL语言的语法太复杂。比如我们有两张表格，一张是学生的学号与其性别，另一张表格记录了每个学生选修的课程。那如果想知道有多少女生选修了某门课程，我们需要先将两张表格联合(JOIN)，再对结果进行过滤(WHERE)，最后进行聚合统计(COUNT)。这个问题在多表的场景中尤为突出，每张表格互相之间通过外键相互关联。其实呢，如果我们把表格中的Header看作各个结点，表格内的结点之间存在联系，而外键可以视作一种特殊的边，这样就可以构成一张图，正如下图中部所示：</p><p>[<img src="/%E6%BC%AB%E8%B0%88%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B%E4%B8%80/o_image-8-ggnn-example2.png" alt="$\pmb g$NN语义解析实例">](<a href="https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-8-$/pmb">https://images.cnblogs.com/cnblogs_com/SivilTaram/1510485/o_image-8-$\pmb</a> g$nn-example2.png)</p><p>论文[11]就是利用了表格这样的特性，利用GGNN来解决多表问题。下面我们先介绍一下一般的语义解析方法，再介绍[11]是如何将图跟语义解析系统联系在一起的。就笔者知道的而言，目前绝大部分语义解析会遵循Seq2seq(序列到序列，Sequence to sequence)的框架，输入是一个个自然语言单词，输出是一个个SQL单词。但这样的框架完全没有考虑到表格对SQL输出暗含的约束。比如说，在单个SELECT子句中，我们选择的若干Header都要来自同一张表。再举个例子，能够JOIN的两张表一定存在外键的联系，就像我们刚刚举的那个学生选课的例子一样。</p><p>那么，GGNN该如何结合到传统的语义解析方法中去呢？在论文[11]中，是通过三步来完成的：</p><ol><li>首先，通过表格建立对应的Graph。再利用GGNN的方法计算每个Header的隐藏状态。</li><li>然后，在Seq2seq模型的编码阶段(Encoding)，用每个输入的自然语言单词的词向量对表格所有Header的隐藏状态算一个Attention，利用Attention作为权重得到了每个自然语言单词的图感知的表示。</li><li>在解码阶段(Decoding)，如果输出的是表格中的Header&#x2F;Table这类词，就用输出的向量与表格所有Header&#x2F;Table的隐藏状态算一个分数，这个分数由$\pmb F$提供的。$\pmb F$实际上是一个全连接层，它的输出实际上是SQL单词与表格中各个Header&#x2F;Table的联系程度。为了让SQL的每个输出都与历史的信息一致，每次输出时都用之前输出的Header&#x2F;Table对候选集中的Header&#x2F;Table打分，这样就利用到了多表的信息。</li></ol><p>最终该论文在多表上的效果也确实很好，下面放一个在Spider[12]数据集上的性能对比：</p><table><thead><tr><th>Model</th><th>A$\pmb c$</th><th>Single</th><th>Multi</th></tr></thead><tbody><tr><td>No GNN</td><td>34.9%</td><td>52.3%</td><td>14.6%</td></tr><tr><td>GNN</td><td><strong>40.7%</strong></td><td>52.2%</td><td><strong>26.8%</strong></td></tr></tbody></table><h3 id="GNN与-pmb-g-NN-https-www-cnblogs-com-SivilTaram-p-graph-neural-network-1-html-gnn与-pmb-g-nn"><a href="#GNN与-pmb-g-NN-https-www-cnblogs-com-SivilTaram-p-graph-neural-network-1-html-gnn与-pmb-g-nn" class="headerlink" title="GNN与$\pmb g$NN[#](https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#gnn与$\pmb g$nn)"></a>GNN与$\pmb g$NN[#](<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#gnn%E4%B8%8E$\pmb">https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#gnn与$\pmb</a> g$nn)</h3><p>GGNN目前得到了广泛的应用，相比于GNN，其最大的区别在于不再以不动点理论为基础，虽然这意味着不再需要迭代收敛，但同时它也意味着GGNN的初始化很重要。从笔者阅读过的文献来看，GNN后的大部分工作都转向了将GNN向传统的RNN&#x2F;CNN靠拢，可能的一大好处是这样可以不断吸收来自这两个研究领域的改进。但基于原始GNN的基于不动点理论的工作非常少，至少在笔者看文献综述的时候并未发现很相关的工作。</p><p>但从另一个角度来看，虽然GNN与GGNN的理论不同，但从设计哲学上来看，它们都与循环神经网络的设计类似。</p><ul><li>循环神经网络的好处在于能够处理任意长的序列，但它的计算必须是串行计算若干个时间步，时间开销不可忽略。所以，上面两种基于循环的图神经网络在更新隐藏状态时不太高效。如果借鉴深度学习中堆叠多层的成功经验，我们有足够的理由相信，<strong>多层图神经网络</strong>能达到同样的效果。</li><li>基于循环的图神经网络每次迭代时都共享同样的参数，而多层神经网络每一层的参数不同，可以看成是一个<strong>层次化特征抽取</strong>(Hierarchical Feature Extraction)的方法。</li></ul><p>而在<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_2.html">下一篇博客</a>中，我们将介绍图卷积神经网络。它摆脱了基于循环的方法，开始走向<strong>多层图神经网络</strong>。在多层神经网络中，<strong>卷积神经网络</strong>(比如152层的ResNet)的大获成功又验证了其在堆叠多层上训练的有效性，所以近几年图卷积神经网络成为研究热点。</p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献#"></a>参考文献<a href="https://www.cnblogs.com/SivilTaram/p/graph_neural_network_1.html#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE">#</a></h2><p>[1]. A Comprehensive Survey on Graph Neural Networks, <a href="https://arxiv.org/abs/1901.00596">https://arxiv.org/abs/1901.00596</a></p><p>[2]. The graph neural network model, <a href="https://persagen.com/files/misc/scarselli2009graph.pdf">https://persagen.com/files/misc/scarselli2009graph.pdf</a></p><p>[3]. Spectral networks and locally connected networks on graphs, <a href="https://arxiv.org/abs/1312.6203">https://arxiv.org/abs/1312.6203</a></p><p>[4]. Distributed Representations of Words and Phrases and their Compositionality, <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases">http://papers.nips.cc/paper/5021-distributed-representations-of-words-andphrases</a></p><p>[5]. DeepWalk: Online Learning of Social Representations, <a href="https://arxiv.org/abs/1403.6652">https://arxiv.org/abs/1403.6652</a></p><p>[6]. Translating Embeddings for Modeling Multi-relational Data, <a href="https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data">https://papers.nips.cc/paper/5071-translating-embeddings-for-modeling-multi-relational-data</a></p><p>[7]. Deep Learning on Graphs: A Survey, <a href="https://arxiv.org/abs/1812.04202">https://arxiv.org/abs/1812.04202</a></p><p>[8]. 如何理解Graph Convolutional Network（GCN）? <a href="https://www.zhihu.com/question/54504471">https://www.zhihu.com/question/54504471</a></p><p>[9]. Almeida–Pineda recurrent backpropagation, <a href="https://www.wikiwand.com/en/Almeida%E2%80%93Pineda_recurrent_backpropagation">https://www.wikiwand.com/en/Almeida–Pineda_recurrent_backpropagation</a></p><p>[10]. Gated graph sequence neural networks, <a href="https://arxiv.org/abs/1511.05493">https://arxiv.org/abs/1511.05493</a></p><p>[11]. Representing Schema Structure with Graph Neural Networks for Text-to-SQL Parsing, <a href="https://arxiv.org/abs/1905.06241">https://arxiv.org/abs/1905.06241</a></p><p>[12]. Spider1.0 Yale Semantic Parsing and Text-to-SQL Challenge, <a href="https://yale-lily.github.io/spider">https://yale-lily.github.io/spider</a></p><p>[13]. <a href="https://www.wikiwand.com/en/Laplacian_matrix">https://www.wikiwand.com/en/Laplacian_matrix</a></p><p>[14]. Graph Neural Networks: A Review of Methods and Applications, <a href="https://arxiv.org/pdf/1812.08434">https://arxiv.org/pdf/1812.08434</a></p>]]></content>
    
    
    <categories>
      
      <category>图神经网络</category>
      
    </categories>
    
    
    <tags>
      
      <tag>图神经网络</tag>
      
      <tag>图卷积</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论战斗唯物主义</title>
    <link href="/2022/05/17/%E8%AE%BA%E6%88%98%E6%96%97%E5%94%AF%E7%89%A9%E4%B8%BB%E4%B9%89/"/>
    <url>/2022/05/17/%E8%AE%BA%E6%88%98%E6%96%97%E5%94%AF%E7%89%A9%E4%B8%BB%E4%B9%89/</url>
    
    <content type="html"><![CDATA[<ol><li>如果共产党员（以及所有成功地开始了大革命的革命家）以为单靠革命家的手就能完成革命事业，那将是他们最大最危险的错误之一。恰恰相反，要使任何一件重大的革命工作得到成功，就必须懂得，革命家只能起真正富有生命力的先进阶级的先锋队的作用，必须善于实现这一点。先锋队只有当它不脱离自己领导的群众并真正引导全体群众前进时，才能完成其先锋队的任务。在各种活动领域中，不同非共产党员结成联盟，就根本谈不上什么有成效的共产主义建设。</li><li>无论如何，我们俄国还有——而且在相当长的时期内无疑还会有——非共产党员的唯物主义者，而吸收一切拥护彻底的战斗唯物主义的人来共同反对哲学上的反动，反对所谓“有教养社会”的种种哲学偏见，是我们不可推委的责任。</li><li>一个马克思主义者如果以为，被整个现代社会置于愚昧无知和囿于偏见这种境地的亿万人民群众（特别是农民和手工业者）只有通过纯粹马克思主义的教育这条直路，才能摆脱愚昧状态，那就是最大的而且是最坏的错误。应该向他们提供各种无神论的宣传材料，告诉他们实际生活各个方面的事实，用各种办法接近他们，以引起他们的兴趣，唤醒他们的宗教迷梦，用种种方法从各方面使他们振作起来，如此等等。</li><li>共产党员和一切彻底的唯物主义者虽然在一定程度上要同资产阶级中的进步分子结成联盟，但是当这些进步分子变成反动的时候，就要坚决地揭露他们。</li><li>这样来研究、解释和宣传黑格尔辩证法是非常困难的，因此，这方面的初步尝试不免要犯一些错误。但是，只有什么事也不做的人才不会犯错误。</li><li>只要愿意学习，就一定能够学会。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论国家与革命</title>
    <link href="/2022/05/17/guojiageming/"/>
    <url>/2022/05/17/guojiageming/</url>
    
    <content type="html"><![CDATA[<ol><li>国家是阶级矛盾不可调和的产物和表现。在阶级矛盾客观上不能调和的地方、时候和条件下，便产生国家。反过来说，国家的存在证明阶级矛盾不可调和。</li><li>既然国家是阶级矛盾不可调和的产物，既然它是站在社会之上并且“日益同社会相异化”的力量，那么很明显，被压迫阶级要求得解放，不仅非进行暴力革命不可，而且非消灭统治阶级所建立的、体现这种“异化”的国家政权机构不可。</li><li>“财富”的无限权力在民主共和制下更可靠，是因为它不依赖政治机构的某些缺陷，不依赖资本主义的不好的政治外壳。民主共和制是资本主义所能采用的最好的政治外壳，所以资本一掌握（通过帕尔钦斯基、切尔诺夫、策列铁里之流）这个最好的外壳，就能十分巩固十分可靠地确立自己的权力，以致在资产阶级民主共和国中，无论人员、无论机构、无论政党的任何更换，都不会使这个权力动摇。</li><li>他们自己相信而且要人民也相信这种荒谬的想法：普选制“在现今的国家里”能够真正体现大多数劳动者的意志，并保证实现这种意志。</li><li>实际上恩格斯在这里所讲的是以无产阶级革命来“消灭”资产阶级的国家，而他讲的自行消亡是指社会主义革命以后无产阶级国家制度残余。按恩格斯的看法，资产阶级国家不是“自行消亡”的，而是由无产阶级在革命中来“消灭”的。在这个革命以后，自行消亡的是无产阶级的国家或半国家。</li><li>资产阶级的国家只有革命才能“消灭”。国家本身，就是说最完全的民主，只能“自行消亡”。</li><li>我们赞成民主共和国，因为这是在资本主义制度下对无产阶级最有利的国家形式。但是，我们决不应该忘记，即使在最民主的资产阶级共和国里，人民仍然摆脱不了当雇佣奴隶的命运。其次，任何国家都是对被压迫阶级“实行镇压的特殊力量”。因此任何国家都不是自由的，都不是人民的。</li><li>马克思和恩格斯关于暴力革命不可避免的学说是针对资产阶级国家说的。资产阶级国家由无产阶级国家（无产阶级专政）代替，不能通过“自行消亡”，根据一般规律，只能通过暴力革命。</li><li>无产阶级国家代替资产阶级国家，非通过暴力革命不可。无产阶级国家的消灭，即任何国家的消灭，只能通过“自行消亡”。</li><li>马克思认为，第一，无产阶级所需要的只是逐渐消亡的国家，即组织得能立刻开始消亡而且不能不消亡的国家；第二，劳动者所需要的“国家”，“即组织成为统治阶级的无产阶级”。</li><li>国家是特殊的强力组织，是镇压某一个阶级的暴力组织。无产阶级要镇压的究竟是哪一个阶级呢？当然只是剥削阶级，即资产阶级。劳动者需要国家只是为了镇压剥削者的反抗，而能够领导和实行这种镇压的只有无产阶级，因为无产阶级是唯一彻底革命的阶级，是唯一能够团结一切被剥削劳动者对资产阶级进行斗争、把资产阶级完全铲除的阶级。</li><li>剥削阶级需要政治统治是为了维持剥削，也就是为了极少数人的私利，去反对绝大多数人。被剥削阶级需要政治统治是为了彻底消灭一切剥削，也就是为了绝大多数人的利益，去反对极少数的现代奴隶主——地主和资本家。</li><li>阶级斗争学说经马克思运用到国家和社会主义革命问题上，必然导致承认无产阶级的政治统治，无产阶级的专政，即不与任何人分掌而直接依靠群众武装力量的政权。只有使无产阶级转化成统治阶级，从而能把资产阶级必然要进行的拼死反抗镇压下去，并组织一切被剥削劳动群众去建立新的经济结构，才能推翻资产阶级。</li><li>马克思主义教育工人的党，也就是教育无产阶级的先锋队，使它能够夺取政权并引导全体人民走向社会主义，指导并组织新制度，成为所有被剥削劳动者在不要资产阶级并反对资产阶级而建设自己社会生活的事业中的导师、领导者和领袖。反之，现在占统治地位的机会主义却把工人的党教育成为一群脱离群众而代表工资优厚的工人的人物，只图在资本主义制度下“苟且偷安”，为了一碗红豆汤而出卖自己的长子权[9]，也就是放弃那领导人民反对资产阶级的革命领袖作用。</li><li>过去一切革命都是使国家机器更加完备，而这个机器是必须打碎，必须摧毁的。</li><li>无产阶级如果不先夺取政权，不取得政治统治，不把国家变为“组织成为统治阶级的无产阶级”，就不能推翻资产阶级；这个无产阶级国家在它取得胜利以后就会立刻开始消亡，因为在没有阶级矛盾的社会里，国家是不需要的，也是不可能存在的。</li><li>谁要是仅仅承认阶级斗争，那他还不是马克思主义者，他还可以不超出资产阶级思想和资产阶级政治的范围。把马克思主义局限于阶级斗争学说，就是阉割马克思主义，歪曲马克思主义，把马克思主义变为资产阶级可以接受的东西。只有承认阶级斗争、同时也承认无产阶级专政的人，才是马克思主义者。</li><li>这个时期必然是阶级斗争空前残酷、阶级斗争的形式空前尖锐的时期，因而这个时期的国家就不可避免地应当是新型民主的（对无产者和一般穷人是民主的）和新型专政的（对资产阶级是专政的）国家。</li><li>马克思不仅是为“冲天的”（他的用语）公社战士的英雄主义感到欢欣鼓舞，他还从这次群众性的革命运动（虽然它没有达到目的）中看到了有极重大意义的历史经验，看到了全世界无产阶级革命的一定进步，看到了比几百种纲领和议论更为重要的实际步骤。分析这个经验，从这个经验中得到策略教训，根据这个经验来重新审查自己的理论，这就是马克思为自己提出的任务。马克思认为对《共产党宣言》必须作的唯一“修改”，就是他根据巴黎公社战士的革命经验作出的。</li><li>无产阶级组织成为统治阶级会采取什么样的具体形式，究竟怎样才能组织得同最完全最彻底地“争得民主”这点相适应，对于这个问题，马克思并没有陷于空想，而是期待群众运动的经验来解答。</li><li>每隔几年决定一次究竟由统治阶级中的什么人在议会里镇压人民、压迫人民，——这就是资产阶级议会制的真正本质，不仅在议会制的立宪君主国内是这样，而且在最民主的共和国内也是这样。</li><li>　摆脱议会制的出路，当然不在于取消代表机构和选举制，而在于把代表机构由清谈馆变为“工作”机构。“公社不应当是议会式的，而应当是工作的机构，兼管行政和立法的机构。”</li><li>我们不是空想主义者。我们并不“幻想”一下子就可以不要任何管理，不要任何服从；这种由于不懂得无产阶级专政的任务而产生的无政府主义幻想，与马克思主义根本不相容，实际上只会把社会主义革命拖延到人们变成另一种人的时候。我们不是这样，我们希望由现在的人来实行社会主义革命，而现在的人没有服从、没有监督、没有“监工和会计”是不行的。但是所需要的服从，是对一切被剥削劳动者的武装先锋队——无产阶级的服从。</li><li>无产阶级和贫苦农民把国家政权掌握在自己手中，十分自由地按公社体制组织起来，把所有公社的行动统一起来去打击资本，粉碎资本家的反抗，把铁路、工厂、土地以及其他私有财产交给整个民族、整个社会，难道这不是集中制吗？难道这不是最彻底的民主集中制、而且是无产阶级的集中制吗？</li><li>空想主义者致力于“发现”可以对社会进行社会主义改造的各种政治形式。无政府主义者根本不考虑政治形式问题。现代社会民主党内的机会主义者则把议会制民主国家的资产阶级政治形式当作不可逾越的极限，对这个“典范”崇拜得五体投地，宣布摧毁这些形式的任何意图都是无政府主义。</li><li>马克思从社会主义和政治斗争的全部历史中得出结论：国家一定会消失；国家消失的过渡形式（从国家到非国家的过渡），将是“组织成为统治阶级的无产阶级”。但是，马克思并没有去发现这个未来的政治形式。他只是对法国历史作了精确的观察，对它进行了分析，得出了1851年所导致的结论：事情已到了破坏资产阶级的国家机器的地步。</li><li>国家会随着阶级的废除而废除，马克思主义向来就是这样教导我们的。《反杜林论》的那段人所共知的关于“国家消亡”的论述，并不是简单地斥责无政府主义者主张废除国家，而是斥责他们鼓吹可以“在一天之内”废除国家。</li><li>这里抓住了对现代资本主义即帝国主义的理论评价中最主要的东西，即资本主义转化为垄断资本主义。后面这四个字必须用黑体加以强调，因为目前最普遍的一种错误就是资产阶级改良主义者所断言的什么垄断资本主义或国家垄断资本主义已经不是资本主义，已经可以称为“国家社会主义”。</li><li>民主共和国是走向无产阶级专政的捷径。因为这样的共和国虽然丝毫没有消除资本的统治，因而也丝毫没有消除对群众的压迫和阶级斗争，但是，它必然会使这个斗争扩大、展开、明朗化和尖锐化，以致一旦出现满足被压迫群众的根本利益的可能性，这种可能性就必然通过而且只有通过无产阶级专政即无产阶级对这些群众的领导得到实现。</li><li>恩格斯同马克思一样，从无产阶级和无产阶级革命的观点出发坚持民主集中制，坚持单一而不可分的共和国。</li><li>我们在向往社会主义的同时深信：社会主义将发展为共产主义，而对人们使用暴力，使一个人服从另一个人、使一部分居民服从另一部分居民的任何必要也将随之消失，因为人们将习惯于遵守公共生活的起码规则，而不需要暴力和服从。</li><li>从前，问题的提法是这样的：无产阶级为了求得自身的解放，应当推翻资产阶级，夺取政权，建立自己的革命专政。现在，问题的提法已有些不同了：从向着共产主义发展的资本主义社会过渡到共产主义社会，非经过一个“政治上的过渡时期”不可，而这个时期的国家只能是无产阶级的革命专政。</li><li>极少数人享受民主，富人享受民主，——这就是资本主义社会的民主制度。如果仔细地考察一下资本主义民主的结构，那么无论在选举权的一些“微小的”（似乎是微小的）细节上（居住年限、妇女被排斥等等），或是在代表机构的办事手续上，或是在行使集会权的实际障碍上（公共建筑物不准“叫化子”使用！），或是在纯粹资本主义的办报原则上，等等，到处都可以看到对民主制度的重重限制。用来对付穷人的这些限制、例外、排斥、阻碍，看起来似乎是很微小的，特别是在那些从来没有亲身体验过贫困、从来没有接近过被压迫阶级群众的生活的人（这种人在资产阶级的政论家和政治家中，如果不占百分之九十九，也得占十分之九）看起来是很微小的，但是这些限制加在一起，就把穷人排斥和推出政治生活之外，使他们不能积极参加民主生活。</li><li>而无产阶级专政，即被压迫者先锋队组织成为统治阶级来镇压压迫者，不能仅仅只是扩大民主。除了把民主制度大规模地扩大，使它第一次成为穷人的、人民的而不是富人的民主制度之外，无产阶级专政还要对压迫者、剥削者、资本家采取一系列剥夺自由的措施。为了使人类从雇佣奴隶制下面解放出来，我们必须镇压这些人，必须用强力粉碎他们的反抗，——显然，凡是实行镇压和使用暴力的地方，也就没有自由，没有民主。</li><li>马克思说：这里确实有“平等的权利”，但这仍然是“资产阶级权利”，这个“资产阶级权利”同任何权利一样，是以不平等为前提的。任何权利都是把同一标准应用在不同的人身上，即应用在事实上各不相同、各不同等的人身上，因而“平等的权利”就是破坏平等，就是不公平。的确，每个人付出与别人同等份额的社会劳动，就能领取同等份额的社会产品（作了上述各项扣除之后）。然而各个人是不同等的：有的强些，有的弱些；有的结了婚，有的没有结婚，有的子女多些，有的子女少些，如此等等。</li><li>可见，在共产主义第一阶段还不能做到公平和平等，因为富裕的程度还会不同，而不同就是不公平。但是人剥削人已经不可能了，因为已经不能把工厂、机器、土地等生产资料攫为私有了。</li><li>马克思不仅极其准确地估计到了人们不可避免的不平等，而且还估计到：仅仅把生产资料转归全社会公有（通常所说的“社会主义”）还不能消除分配方面的缺点和“资产阶级权利”的不平等，只要产品“按劳动”分配，“资产阶级权利”就会继续通行。</li><li>因此，在共产主义社会的第一阶段（通常称为社会主义），“资产阶级权利”没有完全取消，而只是部分地取消，只是在已经实现的经济变革的限度内取消，即只是在同生产资料的关系上取消。“资产阶级权利”承认生产资料是个人的私有财产。而社会主义则把生产资料变为公有财产。在这个范围内，也只是在这个范围内，“资产阶级权利”才不存在了。</li><li>还有国家的时候就没有自由。到有自由的时候就不会有国家了。</li><li>在第一阶段，共产主义在经济上还不可能完全成熟，完全摆脱资本主义的传统或痕迹。由此就产生一个有趣的现象，这就是在共产主义第一阶段还保留着“资产阶级权利的狭隘眼界”。既然在消费品的分配方面存在着资产阶级权利，那当然一定要有资产阶级国家，因为如果没有一个能够强制人们遵守权利准则的机构，权利也就等于零。　可见，在共产主义下，在一定的时期内，不仅会保留资产阶级权利，甚至还会保留资产阶级国家，——但没有资产阶级！</li><li>因为当所有的人都学会了管理，都来实际地独立地管理社会生产，对寄生虫、老爷、骗子等等“资本主义传统的保持者”独立地进行计算和监督的时候，逃避这种全民的计算和监督就必然会成为极难得逞的、极罕见的例外，可能还会受到极迅速极严厉的惩罚（因为武装工人是重实际的人，而不是重感情的知识分子；他们未必会让人跟自己开玩笑），以致人们对于人类一切公共生活的简单的基本规则就会很快从必须遵守变成习惯于遵守了。　　到那时候，从共产主义社会的第一阶段过渡到它的高级阶段的大门就会敞开，国家也就随之完全消亡。</li><li>在资本主义下，由于雇佣奴隶制和群众贫困的整个环境，民主制度受到束缚、限制、阉割和弄得残缺不全。因为这个缘故，而且仅仅因为这个缘故，我们政治组织和工会组织内的公职人员是受到了资本主义环境的腐蚀（确切些说，有被腐蚀的趋势），是有变为官僚的趋势，也就是说，是有变为脱离群众、站在群众之上、享有特权的人物的趋势。</li><li>这就是官僚制的实质，在资本家被剥夺以前，在资产阶级被推翻以前，甚至无产阶级的公职人员也免不了在一定程度上“官僚化”。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>哥达纲领批判-笔记</title>
    <link href="/2022/05/17/gedagangling/"/>
    <url>/2022/05/17/gedagangling/</url>
    
    <content type="html"><![CDATA[<ol><li>劳动不是一切财富的源泉。自然界和劳动一样也是使用价值（而物质财富本来就是由使用价值构成的！）的源泉，劳动本身不过是一种自然力的表现，即人的劳动力的表现。但是这句话只是在它包含着劳动具备了相应的对象和资料这层意思的时候才是正确的。</li><li>一个除自己的劳动力外没有任何其他财产的人，在任何社会的和文化的状态中，都不得不为占有劳动的物质条件的他人做奴隶。他只有得到他人的允许才能劳动，因而只有得到他人的允许才能生存。</li><li>每一个生产者，在作了各项扣除之后，从社会方面正好领回他所给予社会的一切。他所给予社会的，就是他个人的劳动量。例如，社会劳动日是由所有的个人劳动小时构成的；每一个生产者的个人劳动时间就是社会劳动日中他所提供的部分，就是他在社会劳动日里的一分。他从社会方面领得一张证书，证明他提供了多少劳动（扣除他为社会基金而进行的劳动），而他凭这张证书从社会储存中领得和他所提供的劳动量相当的一分消费资料。他以一种形式给予社会的劳动量，又以另一种形式全部领回来。</li><li>在这里平等的权利按照原则仍然是资产阶级的法权，虽然原则和实践在这里已不再互相矛盾，而在商品交换中，等价物的交换只存在于平均数中，并不是存在于每个个别场合。</li><li>　　虽然有这种进步，但这个平等的权利还仍然被限制在一个资产阶级的框框里。生产者的权利是和他们提供的劳动成比例的；平等就在于以同一的尺度——劳动——来计量。</li><li>一个人在体力或智力上胜过另一个人，因此在同一时间内提供较多的劳动，或者能够劳动较长的时间；而劳动，为了要使它能够成为一种尺度，就必须按照它的时间或强度来确定，不然它就不成其为尺度了。这种平等的权利，对不同等的劳动来说是不平等的权利。</li><li>但是这些弊病，在共产主义社会第一阶段，在它经过长久的阵痛刚刚从资本主义社会里产生出来的形态中，是不可避免的。权利永远不能超出社会的经济结构以及由经济结构所制约的社会的文化发展。</li><li>在共产主义社会高级阶段上，在迫使人们奴隶般地服从分工的情形已经消失，从而脑力劳动和体力劳动的对立也随之消失之后；在劳动已经不仅仅是谋生的手段，而且本身成了生活的第一需要之后；在随着个人的全面发展生产力也增长起来，而集体财富的一切源泉都充分涌流之后，——只有在那个时候，才能完全超出资产阶级法权的狭隘眼界，社会才能在自己的旗帜上写上：各尽所能，按需分配！</li><li>物质的生产条件以资本和地产的形式掌握在非劳动者的手中，而人民大众则只有人身的生产条件，即劳动力。</li><li>为了能够进行斗争，工人阶级必须在国内组成为一个阶级，而且它的直接的斗争舞台就是本国，这是不言而喻的。所以，它的阶级斗争不是就内容来说，而是像“共产党宣言”所指出“就形式来说”是本国范围内的斗争。</li><li>雇佣工人只有为资本家（因而也为他们的剩余价值的分享者）白白地劳动一定的时间，才被允许为维持自己的生活而劳动，就是说，才被允许生存；整个资本主义生产体系的中心问题就在于：用延长劳动日的办法，或者用提高生产率、从而使劳动力更加紧张的办法等等，来增加这个无偿劳动；因此，雇佣劳动制度是奴隶制度，而且社会劳动生产力愈发展，这种奴隶制度就愈残酷，不管工人得到的报酬较好或是较坏。</li><li>自由就在于把国家由一个站在社会之上的机关变成完全服从这个社会的机关。</li><li>在资本主义社会和共产主义社会之间，有一个从前者变为后者的革命转变时期。同这个时期相适应的也有一个政治上的过渡时期，这个时期的国家只能是无产阶级的革命专政。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>经济学哲学手稿笔记1844</title>
    <link href="/2022/05/14/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%93%B2%E5%AD%A6%E6%89%8B%E7%A8%BF%E7%AC%94%E8%AE%B01844/"/>
    <url>/2022/05/14/%E7%BB%8F%E6%B5%8E%E5%AD%A6%E5%93%B2%E5%AD%A6%E6%89%8B%E7%A8%BF%E7%AC%94%E8%AE%B01844/</url>
    
    <content type="html"><![CDATA[<h1 id="《1844经济学哲学手稿笔记》马克思"><a href="#《1844经济学哲学手稿笔记》马克思" class="headerlink" title="《1844经济学哲学手稿笔记》马克思"></a>《1844经济学哲学手稿笔记》马克思</h1><ol><li>仔细考察起来，神学的批判——尽管在运动之初曾是一个真正的进步因素——归根结底不外是旧哲学的、特别是黑格尔的超验性被歪曲为神学漫画的顶点和结果。历史现在仍然指派神学这个历来的哲学的溃烂区本身来显示哲学的消极解体，即哲学的腐烂过程。</li><li>工资决定于资本家和工人之间的敌对的斗争。胜利必定属于资本家。资本家没有工人能比工人没有资本家活得长久。资本家的联合是常见的和有效的，工人的联合则遭到禁止并会给他们招来恶果。</li><li>对人的需求必然调节人的生产，正如其他任何商品生产的情况一样。如果供给大大超过需求，那么一部分工人就要沦为乞丐或者饿死。因此，工人的存在被归结为其他任何商品的存在条件。工人成了商品，如果他能找到买主，那就是他的幸运了。工人的生活取决于需求，而需求取决于富人和资本家的兴致。</li><li>当资本家赢利时工人不一定有利可得，而当资本家亏损时工人就一定跟着吃亏。</li><li>劳动价格要比生活资料的价格远为稳定。二者往往成反比。在物价腾贵的年代，工资因对劳动的需求下降而下降，因生活资料价格提高而提高。这样就互相抵消。无论如何，总有一定数量的工人没有饭吃。在物价便宜的年代，工资因对劳动的需求提高而提高，因生活资料价格下降而下降。这样也就互相抵消。</li><li>总之，应当看到，工人和资本家同样苦恼，工人是为他的生存而苦恼，资本家则是为他的死钱财的赢利而苦恼。工人不仅必须为物质的生活资料而斗争，而且必须为谋求工作，即为谋求实现自己的活动的可能性、手段而斗争。</li><li>大量劳动积累起来，因为资本是积累的劳动。就是说，工人的劳动产品越来越多地从他手中被拿走，工人自己的劳动越来越作为别人的财产同他相对立，而他的生存资料和活动资料越来越多地积聚在资本家手中。</li><li>资本的积累扩大分工，而分工则增加工人的人数；反过来，工人人数的增加扩大分工，而分工又增加资本的积累。一方面随着分工的扩大，另一方面随着资本的积累，工人日益完全依赖于劳动，依赖于一定的、极其片面的、机器般的劳动。这样，随着工人在精神上和肉体上被贬低为机器，随着人变成抽象的活动和胃，工人也越来越依赖于市场价格的一切波动，依赖于资本的使用和富人的兴致。同时，由于单靠劳动为生者阶级的人数增加，工人之间的竞争加剧了，因而他们的价格也降低了。在工厂制度下工人的这种状况达到了顶点。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>雇佣劳动与资本笔记</title>
    <link href="/2022/05/14/%E9%9B%87%E4%BD%A3%E5%8A%B3%E5%8A%A8%E4%B8%8E%E8%B5%84%E6%9C%AC%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/05/14/%E9%9B%87%E4%BD%A3%E5%8A%B3%E5%8A%A8%E4%B8%8E%E8%B5%84%E6%9C%AC%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="《雇佣劳动与资本》笔记"><a href="#《雇佣劳动与资本》笔记" class="headerlink" title="《雇佣劳动与资本》笔记"></a>《雇佣劳动与资本》笔记</h1><ol><li>工资只是劳动价格［注：在1891年的版本中，“劳动价格”改为“通常被称为劳动价格的劳动力价格”。——编者注］的特种名称.</li><li>劳动是工人本身的生命活动，是工人本身的生命的表现。工人正是把这种生命活动出卖给别人，以获得自己所必需的生活资料。可见，工人的生命活动对于他不过是使他能以生存的一种手段而已。他是为生活而工作的。他甚至不认为劳动是自己生活的一部分；相反地，对于他来说，劳动就是牺牲自己的生活。劳动是已由他出卖给别人的一种商品。因此，他的活动的产物也就不是他的活动的目的。</li><li>他能不能认为这十二小时的织布、纺纱、钻孔、研磨、建筑、挖掘、打石子是他的生活的表现，是他的生活呢？恰恰相反，对于他来说，在这种活动停止以后，当他坐在饭桌旁，站在酒店柜台前，睡在床上的时候，生活才算开始。</li><li>劳动并不向来就是商品。劳动并不向来就是雇佣劳动、即自由劳动。 </li><li>工人是以出卖劳动为其工资的唯一来源的，如果他不愿饿死，就不能离开整个购买者阶级即资本家阶级。工人不是属于某一个资产者，而是属于整个资产阶级。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>神圣家族笔记</title>
    <link href="/2022/05/14/%E7%A5%9E%E5%9C%A3%E5%AE%B6%E6%97%8F%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/05/14/%E7%A5%9E%E5%9C%A3%E5%AE%B6%E6%97%8F%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="《神圣家族，或对批判的批判所做的批判》马恩"><a href="#《神圣家族，或对批判的批判所做的批判》马恩" class="headerlink" title="《神圣家族，或对批判的批判所做的批判》马恩"></a>《神圣家族，或对批判的批判所做的批判》马恩</h1><ol><li>价值纯粹是偶然确定的，它无论和生产费用或者和社会效用都没有任何关系。工资的数额起初是由自由的工人和自由的资本家自由协商来确定的。后来却发现，工人是被迫同意资本家所规定的工资，而资本家则是被迫把工资压到尽可能低的水平。强制代替了立约双方的自由。</li><li>无产阶级和富有是两个对立面。它们本身构成一个统一的整体。它们二者都是由私有制世界产生的。问题在于这两个方面中的每一个方面在对立中究竟占有什么样的确定的地位。只宣布它们是统一整体的两个方面是不够的。私有制，作为私有制来说，作为富有来说，不能不保持自身的存在，因而也就不能不保持自己的对立面——无产阶级的存在。这是对立的肯定方面，是得到自我满足的私有制。相反地，无产阶级，作为无产阶级来说，不能不消灭自身，因而也不能不消灭制约着它而使它成为无产阶级的那个对立面——私有制。这是对立的否定方面，是对立内部的不安，是已被消灭的并且正在消灭自身的私有制。</li><li>有产阶级和无产阶级同是人的自我异化。但有产阶级在这种自我异化中感到自己是被满足的和被巩固的，它把这种异化看做自身强大的证明，并在这种异化中获得人的生存的外观。而无产阶级在这种异化中则感到自己是被毁灭的，并在其中看到自己的无力和非人的生存的现实。这个阶级，用黑格尔的话来说，就是在被唾弃的状况下对这种状况的愤慨，这个阶级之所以必然产生这种愤慨，是由于它的人类本性和它那种公开地、断然地、全面地否定这种本性的生活状况相矛盾。由此可见，在整个对立的范围内，私有者是保守的方面、无产者是破坏的方面。从前者产生保持对立的行动，从后者则产生消灭对立的行动。</li><li>思辨的思维从各种不同的现实的果实中得出一个抽象的“果实”——“一般果实”，所以为了要达到某种现实内容的假象，它就不得不用这种或那种方法从“果实”、从实体返回到现实的千差万别的平常的果实，返回到梨、苹果、扁桃等等上去。但是，要从现实的果实得出“果实”这个抽象的观念是很容易的，而要从“果实”这个抽象的观念得出各种现实的果实就很困难了。不但如此，要从抽象转到抽象的直接对立面，不抛弃抽象是绝对不可能的。</li><li>因此，我们就不能根据我们从实体观念得出的看法再说梨是“果实”，苹果是“果实”，扁桃是“果实”；相反地应该说“果实”确定自己为梨，“果实”确定自己为苹果，“果实”确定自己为扁桃；苹果、梨、扁桃相互之间的差别，正是“果实”的自我差别，这些差别使各种特殊的果实正好成为“一般果实”生活过程中的千差万别的环节。这样，“果实”就不再是无内容、无差别的统一体，而是作为总和、作为各种果实的“总体”的统一体，这些果实构成一个“被有机地划分为各个环节的系列”。在这个系列的每一个环节中“果实”都使自己得到一种更为发展、更为显著的定在，直到它最后作为一切果实的“概括”，同时成为活生生的统一体。这统一体把单个的果实都消溶于自身中，又从自身生出各种果实，正如人体的各部分不断消溶于血液，又不断从血液中生出一样。</li><li>假定我们有六种动物，譬如说有狮子、鲨鱼、蛇、牛、马和哈巴狗。我们从这六种动物中抽象出“一般动物”这个范畴。把“一般动物”想象为独立的存在物。把狮子、鲨鱼、蛇等等看做“一般动物”的化装或体现。我们既可以把我们的想象的东西，即我们抽象的“动物”变成某种现实的存在物，同样也就可以把现实的动物变成我们抽象的创造物，即我们想象的创造物。我们看见“一般动物”体现为狮子，就会把人撕得粉碎；体现为鲨鱼，就会把人吞下去；体现为蛇，就会用毒液伤人；体现为牛，就会用角ód人；体现为马，就会用蹄子踢人；但是，如果“一般动物”体现为哈巴狗，就只会对人吠叫，并把和人的搏斗完全变成搏斗的外观。</li><li>历史什么事情也没有做，它“并不拥有任何无穷尽的丰富性”，它并“没有在任何战斗中作战”！创造这一切、拥有这一切并为这一切而斗争的，不是“历史”，而正是人，现实的、活生生的人。“历史”并不是把人当做达到自己目的的工具来利用的某种特殊的人格。历史不过是追求着自己目的的人的活动而已。</li><li>唯物主义在它的第一个创始人培根那里，还在朴素的形式下包含着全面发展的萌芽。物质带着诗意的感性光辉对人的全身心发出微笑。但是，用格言形式表述出来的学说本身却反而还充满了神学的不彻底性。</li><li>霍布斯根据培根的观点论断说，如果我们的感觉是我们的一切知识的泉源，那末观念、思想、意念等等，就不外乎是多少摆脱了感性形式的实体世界的幻影。科学只能给这些幻影冠以名称。同一个名称可以适用于许多幻影。甚至还可以有名称的名称。但是，如果一方面认为感性世界是一切观念的泉源，而另一方面又硬说一个词的意义不只是一个词，除了我们想象的永远单一的存在物之外，还有某种普遍的存在物，那就矛盾了。无形体的实体也像无形体的物体一样，是一个矛盾。物体、存在、实体是同一种实在的观念。决不可以把思维同那思维着的物质分开。物质是一切变化的主体。假如“无限的”这个词不表示我们的精神能够无限地添加某一数量，那末这个词就毫无意义。既然只有物质的东西才是可以觉察到的，才是可以认识的，那末对神的存在就丝毫不能有所知了。</li><li>既然人是从感性世界和感性世界中的经验中汲取自己的一切知识、感觉等等，那就必须这样安排周围的世界，使人在其中能认识和领会真正合乎人性的东西，使他能认识到自己是人。既然正确理解的利益是整个道德的基础，那就必须使个别人的私人利益符合于全人类的利益。既然从唯物主义意义上来说人是不自由的，就是说，既然人不是由于有逃避某种事物的消极力量，而是由于有表现本身的真正个性的积极力量才得到自由，那就不应当惩罚个别人的犯罪行为，而应当消灭犯罪行为的反社会的根源，并使每个人都有必要的社会活动场所来显露他的重要的生命力。既然人的性格是由环境造成的，那就必须使环境成为合乎人性的环境。既然人天生就是社会的生物，那他就只有在社会中才能发展自己的真正的天性，而对于他的天性的力量的判断，也不应当以单个个人的力量为准绳，而应当以整个社会的力量为准绳。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>论国家笔记</title>
    <link href="/2022/05/14/%E8%AE%BA%E5%9B%BD%E5%AE%B6%E7%AC%94%E8%AE%B0/"/>
    <url>/2022/05/14/%E8%AE%BA%E5%9B%BD%E5%AE%B6%E7%AC%94%E8%AE%B0/</url>
    
    <content type="html"><![CDATA[<h1 id="《论国家》–列宁"><a href="#《论国家》–列宁" class="headerlink" title="《论国家》–列宁"></a>《论国家》–列宁</h1><ol><li>这个问题也和所有的问题(如资本主义、人对人的剥削怎样产生，社会主义怎样出现，它产生的条件是什么)一样，要正确地分析它，要有把握地切实地解决它，就必须对它的整个发展过程作历史的考察。研究国家问题的时候，首先就要注意，国家不是从来就有的。曾经有过一个时候是没有国家的。国家是在社会划分为阶级的地方和时候、在剥削者和被剥削者出现的时候才出现的。</li><li>那时还没有国家，没有系统地使用暴力和强迫人们服从暴力的特殊机构。这样的机构就叫作国家。</li><li>但是在任何地方我们都看不到一种特殊等级的人分化出来管理他人并为了管理而系统地一贯地掌握着某种强制机构即暴力机构，这种暴力机构，大家知道，现在就是武装队伍、监狱及其他强迫他人意志服从暴力的手段，即构成国家实质的东西。</li><li>当专门从事管理并因此而需要一个强迫他人意志服从暴力的特殊强制机构（监狱、特殊队伍即军队，等等）的特殊集团出现时，国家也就出现了。</li><li>历史告诉我们，国家这种强制人的特殊机构，只是在社会划分为阶级，即划分为这样一些集团，其中一些集团能够经常占有另一些集团的劳动的地方和时候，只是在人剥削人的地方，才产生出来的。</li><li>国家一直是从社会中分化出来的一种机构，是由一批专门从事管理、几乎专门从事管理或主要从事管理的人组成的一种机构。人分为被管理者和专门的管理者，后者高居于社会之上，称为统治者，称为国家代表。这个机构，这个管理别人的集团，总是把持着一定的强制机构，实力机构，不管这种加之于人的暴力表现为原始时代的棍棒，或是奴隶制时代较为完善的武器，或是中世纪出现的火器，或是完全利用现代技术最新成果造成的、堪称20世纪技术奇迹的现代化武器，反正都是一样。使用暴力的手段虽然改变，但是只要国家存在，每个社会就总有一个集团进行管理，发号施令，实行统治，并且为了维持政权而把实力强制机构、其装备同每个时代的技术水平相适应的暴力机构把持在自己手中。</li><li>国家是维护一个阶级对另一个阶级的统治的机器。</li><li>要强迫社会上的绝大多数人经常替另一部分人做工，就非有一种经常性的强制机构不可。当没有阶级的时候，也就没有这种机构。在阶级出现以后，随着阶级划分的加强和巩固，随时随地就有一种特殊的机关即国家产生出来。</li><li>国家是一个阶级压迫另一个阶级的机器，是迫使一切从属的阶级服从于一个阶级的机器。</li><li>国家（资本主义国家）仍然是帮助资本家控制贫苦农民和工人阶级的机器，但它在表面上是自由的。它宣布普选权，并且通过自己的拥护者、鼓吹者、学者和哲学家宣称它不是阶级的国家。</li><li>凡是存在着土地和生产资料的私有制、资本占统治地位的国家，不管怎样民主，都是资本主义国家，都是资本家用来控制工人阶级和贫苦农民的机器。至于普选权、立宪会议和议会，那不过是形式，不过是一种空头支票，丝毫也不能改变事情的实质。</li><li>北美合众国是世界上最民主的共和国之一，可是，世界上没有一个国家象美国那样（凡是在1905年以后到过那里的人大概都知道），资本权力即一小撮亿万富翁统治整个社会的权力表现得如此横蛮，采用贿赂手段如此明目张胆。资本既然存在，也就统治着整个社会，所以任何民主共和制、任何选举制度都不会改变事情的实质。</li><li>资产阶级的共和制、议会和普选制，所有这一切，从全世界社会发展来看，是一大进步。人类走到了资本主义，而只有资本主义，凭借城市的文化，才使被压迫的无产者阶级有可能认清自己的地位，创立世界工人运动，造就出在全世界组织成政党的千百万工人，建立起自觉地领导群众斗争的社会主义政党。没有议会制度，没有选举制度，工人阶级就不会有这样的发展。因此，这一切东西在广大群众的眼中具有很大的意义。</li><li>不管一个共和国用什么形式掩饰起来，就算它是最民主的共和国吧，如果它是资产阶级共和国，如果它那里保存着土地和工厂的私有制，私人资本把全社会置于雇佣奴隶的地位，换句话说，如果它不实现我们党纲和苏维埃宪法所宣布的那些东西，那么这个国家还是一部分人压迫另一部分人的机器。因此要把这个机器夺过来，由必将推翻资本权力的那个阶级来掌握。我们要抛弃一切关于国家就是普遍平等的陈腐偏见，那是骗人的，因为只要剥削存在，就不会有平等。地主不可能同工人平等，挨饿者也不可能同饱食者平等。人们崇拜国家达到了迷信的地步，相信国家是全民政权的陈词滥调；无产阶级就是要扔掉这个叫作国家的机器，并且指出这是资产阶级的谎言。我们已经从资本家那里把这个机器夺了过来，由自己掌握。<strong>我们要用这个机器或者说这根棍棒去消灭一切剥削。到世界上再没有进行剥削的可能，再没有土地占有者和工厂占有者，再没有一部分人吃得很饱而一部分人却在挨饿的现象的时候，就是说，只有到再没有发生这种情形的可能的时候，我们才会把这个机器毁掉。那时就不会有国家了，就不会有剥削了。这就是我们共产党的观点。</strong></li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>意识形态的实践结构</title>
    <link href="/2022/05/14/%E6%84%8F%E8%AF%86%E5%BD%A2%E6%80%81%E7%9A%84%E5%AE%9E%E8%B7%B5%E7%BB%93%E6%9E%84/"/>
    <url>/2022/05/14/%E6%84%8F%E8%AF%86%E5%BD%A2%E6%80%81%E7%9A%84%E5%AE%9E%E8%B7%B5%E7%BB%93%E6%9E%84/</url>
    
    <content type="html"><![CDATA[<h1 id="《意识形态的实践结构及列宁对马克思主义的最独特贡献》笔记-赵凯荣"><a href="#《意识形态的实践结构及列宁对马克思主义的最独特贡献》笔记-赵凯荣" class="headerlink" title="《意识形态的实践结构及列宁对马克思主义的最独特贡献》笔记-赵凯荣"></a>《意识形态的实践结构及列宁对马克思主义的最独特贡献》笔记-赵凯荣</h1><ol><li>马克思主义是理论和实践统一的整体或“整钢”，只有在实践中实现了的理论才被马克思视为理论的“完成”</li><li>没有马克思，社会主义就不能从空想到科学；没有列宁，社会主义就不能从科学成为现实。</li><li>其次是要有自我实现的现实的、物质的实践手段。对于黑格尔而言，这种力量绝对理性总会给自己找到的，当世界历史需要一个叫作拿破仑的人为自己开辟道路时，这个人就出现了。所以，当黑格尔称拿破仑为“骑在马上的世界精神”时，不过是说，拿破仑成了世界精神的理性狡猾的工具。</li><li>对马克思来说，这种力量显然就是“群众”，也只可能是“群众”。群众为什么特别符合意识形态的这个实践结构？因为：“理论一经掌握群众，就会变成物质力量。”</li><li>在市民社会，任何一个阶级要能够扮演这个角色，就必须在自身和群众中激起瞬间的狂热。在这瞬间，这个阶级与整个社会亲如兄弟，汇合起来，与整个社会混为一体并且被看作和被认为是社会的总代表。</li><li>哲学把无产阶级当作自己的物质武器，同样，无产阶级也把哲学当作自己的精神武器……德国人的解放就是人的解放。这个解放的头脑是哲学，它的心脏是无产阶级。哲学不消灭无产阶级，就不能成为现实；无产阶级不把哲学变成现实，就不可能消灭自身。</li><li>只不过从《神圣家族》起，这一结构的内涵不再适应，于是外壳保留了下来，内涵发生了改变，这一结构也就被转换成了“科学 + 无产阶级实践”：</li><li>问题不在于某个无产者或者甚至整个无产阶级暂时提出什么样的目标，问题在于无产阶级究竟是什么，无产阶级由于其身为无产阶级而不得不在历史上有什么作为。它的目标和它的历史使命已经在它自己的生活状况和现代资产阶级社会的整个组织中明显地、无可更改地预示出来了。</li><li>列宁把一些知识分子对所谓公正、客观、科学的追求视为偏执，视为天真和幼稚：因为建筑在阶级斗争上的社会是不可能有“公正的”社会科学的。</li><li>在社会中，产品一经完成，生产者对产品的关系就是一种外在的关系，产品回到主体，取决于主体对其他个人的关系。他不是直接获得产品。</li><li>列宁认为，“帝国主义是资本主义发展的最高阶段”，“就其经济实质来说，是垄断资本主义”</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>马列主义入门书单</title>
    <link href="/2022/05/12/marxism_booklist/"/>
    <url>/2022/05/12/marxism_booklist/</url>
    
    <content type="html"><![CDATA[<h1 id="【转载】马列主义入门小书单"><a href="#【转载】马列主义入门小书单" class="headerlink" title="【转载】马列主义入门小书单"></a>【转载】<a href="https://zhuanlan.zhihu.com/p/375574038">马列主义入门小书单</a></h1><p><a href="https://www.zhihu.com/people/LinkinBryant">林布</a></p><p>写这个是因为之前有人问，然后刚好我在知乎上刷到了一个书单推荐，颇匪夷所思。不过此处不隔空对线了。</p><h2 id="0，基本的方法论"><a href="#0，基本的方法论" class="headerlink" title="0，基本的方法论"></a><strong>0，基本的方法论</strong></h2><p>想要很好地认识马列主义，其实往往并不是直接从哲学性的、思想性的阅读中来的。<br>我个人是在学习历史的过程中开始感受到历史唯物主义和辩证唯物主义的重要性，尤其是其作为方法论、作为在史书之外的角度看待历史的价值。当然，更好的办法是广泛地亲自参与社会实践。</p><p>当然这很漫长，也不适用于所有人。所以我还是要列出一个简单的书单来。对于马列主义经典作者的阅读，必定要秉承着唯物史观的方法论。具体来说就是：经典论述→一般原理→具体现实→具体方法。</p><p>避免教条主义，避免刻舟求剑，但是同样要抓住马列主义最核心的原则。既要避免左灯右转，也要避免原教旨念经。关于马列主义的核心原则，一言蔽之，科学性和革命性。具体不展开说了。我很喜欢列宁援引过的一句话。科学如果不能得到正确的哲学的帮助，就会跌入反动的泥坑。而所谓战斗的唯物主义，（如果脱离了科学的指导），那与其说是战斗，不如说是挨揍。</p><h2 id="1，看书之前，先看一篇论文"><a href="#1，看书之前，先看一篇论文" class="headerlink" title="1，看书之前，先看一篇论文"></a><strong>1，看书之前，先看一篇论文</strong></h2><p>武汉大学赵凯荣教授的《<a href="https://link.zhihu.com/?target=https://kns.cnki.net/kcms/detail/detail.aspx?dbcode=CCJD&dbname=CCJDLAST2&filename=MKZY201701013&v=8vFlCYXUNNVHdPRmUVaiVSngpt4AHKXitlaII%25mmd2FxVAYKjf%25mmd2BfRCMrBbS8%25mmd2Fwi4AKdvT">意识形态的实践结构及列宁对马克思主义的最独特贡献</a>》</p><p>为什么要把这篇论文放在书单的第一篇。是因为马列主义的原作真的比较难啃，而且期间的思想问题的思辨是长期延续的。<br>光看书，而得不到一个正确的核心方向的引领，就很容易陷入到迷茫之中。<br>无论是知乎上的左小将，还是历史上的考茨基。他们看的书不可谓不多。他们做的事，不可谓不错。——好吧有些左小将可能真的没看书。</p><p>而为什么要提这篇论文呢，我总结以下三个原因</p><ul><li><p>论述了马克思的唯物主义“区别于从前的一切唯物主义”的根本要素，就是实践，就是人的实践，也就是马克思所说的“感性的人的活动”。“一条铁路，在被制造出来之前，就已经在意识里存在了。然而不被制造出来，就不足以称为意识形态在实践上的完成”<br>“一条铁路，如果仅仅是被制造出来。而不通车、不被使用、不被磨损，那它也不是真正的铁路，仅仅是可能成为铁路的某个象形”</p></li><li><p>提示了从唯物史观角度学习马列主义的方法论。论述了马克思主义与列宁主义的联系和创新。尤其是，如何在不同的客观条件下，根据旧的论述、总结一般性原理，再结合客观条件去建立新的理论。而这根本上是为了理论在实践层面上的完成。<br>尤其是，这同样论述了马列主义作为一个开放的、发展的理论体系的特征。以及马克思主义与列宁主义的整体性。</p></li><li><p>一个真正有理论高度的学者是如何对观点进行论述的。这当然没有固定的范式。但是你如何去看一篇文章，怎么去认识到这个作者的论述是有高度的、是可信的。我觉得这篇论文可以当范本。</p></li></ul><h2 id="2，这回真的是书单了，第一部分"><a href="#2，这回真的是书单了，第一部分" class="headerlink" title="2，这回真的是书单了，第一部分"></a><strong>2，这回真的是书单了，第一部分</strong></h2><h3 id="2-1，列宁《国家与革命》、马克思《哥达纲领批判》。"><a href="#2-1，列宁《国家与革命》、马克思《哥达纲领批判》。" class="headerlink" title="2.1，列宁《国家与革命》、马克思《哥达纲领批判》。"></a>2.1，列宁《国家与革命》、马克思《哥达纲领批判》。</h3><p>这两篇文章论述的都是马列主义的国家观和社会观，其基本概念基本上可以在中学政治书里看到。类似于专政工具、暴力机关等概念。</p><p>把这两篇放在开头，是因为他对入门者友好。而且其主题，也是那些对马列主义感兴趣的人所关心的。</p><p>而马列主义在其中剥开阶级社会文明、道德的面纱，把其阶级本质暴露出来的做法。既是批判，也为马列主义定下了一个基调，即抛开道德批判，关注于阶级批判与经济批判。</p><h3 id="2-2，列宁《论战斗唯物主义的意义》"><a href="#2-2，列宁《论战斗唯物主义的意义》" class="headerlink" title="2.2，列宁《论战斗唯物主义的意义》"></a>2.2，列宁《论战斗唯物主义的意义》</h3><p>这一篇是比较冷门的。推荐的理由就是短。还有就是可读性比较强。<br>因为列宁是个很杠的人，而且他的政论时时刻刻透出一种刻薄尖锐的味道。而文章的主题和针对的问题也大多比较明确。 而我前面提到的那个“与其说是战斗不如说是挨揍”也是出自一篇。<br>对于入门者来说，读来比较好玩。兴趣比经学重要。更何况，这一篇还重点论述了我认为很重要的科学与哲学的问题。</p><h3 id="2-3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言"><a href="#2-3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言" class="headerlink" title="2.3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言"></a>2.3，马克思《资本论：第一卷》、马克思《德意志意识形态》、马克思《政治经济学批判》序言</h3><p>资本论第一卷只用粗读。反正你以后还是得回来的。<br>涉及到比较抽象的概念，觉得自己看得眼神涣散的，就简单点略过。重点可以看其中的第三章、第六章，概念性的东西不多。主要是陈述性的东西。</p><p>然后反过来看《德意志意识形态》和《政治经济学批判》序言，主要关注唯物史观的部分。</p><h3 id="2-4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》"><a href="#2-4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》" class="headerlink" title="2.4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》"></a>2.4，孙正聿《马克思主义基础理论研究》、陈先达《马克思主义哲学原理》</h3><p>这里提到的两位作者都是国内学界的…老师。他们可能是很多马哲学者老师的老师。<br>实话说，但凡把这两部作品不太囫囵的看完了的话，水平就不知道比我高到哪里去了。</p><p>《马克思主义哲学原理》这部书有很多同名图书，作者都不同。请认准陈先达老师，主要是别的我没看过，不保证质量。</p><p>作为延伸阅读，可以考虑陈先达老师的《马克思主义基础理论若干重大问题研究》</p><h2 id="3，书单的第二部分"><a href="#3，书单的第二部分" class="headerlink" title="3，书单的第二部分"></a><strong>3，书单的第二部分</strong></h2><p>前面的第一部分是解决基本概念、基本思维和基本问题的部分。而第二部分则更侧重于进一步地从历史中去认识马列主义。实际上我自己也还在这个阶段划水，一眼没看到边。只能根据自己的阅读和计划来去重新厘清这些问题。</p><h3 id="3-1，是张一兵教授的《回到马克思》与《回到列宁》"><a href="#3-1，是张一兵教授的《回到马克思》与《回到列宁》" class="headerlink" title="3.1，是张一兵教授的《回到马克思》与《回到列宁》"></a>3.1，是张一兵教授的《回到马克思》与《回到列宁》</h3><p>这两本著作是对马列二人的思想的一个深度梳理，而且能够帮助读者把他们各自的经历、思想脉络串联起来。在这里，插播一条同样是张一兵教授的论文《列宁阅读黑格尔哲学的初始视界》<br>不长，简单一句话：看不懂很正常。张一兵教授的这篇论文说的就是，列宁当时都没看明白，一边疯狂吐槽一边看，看完反思以后继续吐槽。</p><h3 id="3-2，马克思《1844年经济学哲学手稿》、《资本论》第一卷"><a href="#3-2，马克思《1844年经济学哲学手稿》、《资本论》第一卷" class="headerlink" title="3.2，马克思《1844年经济学哲学手稿》、《资本论》第一卷"></a>3.2，马克思《1844年经济学哲学手稿》、《资本论》第一卷</h3><p>资本论第一卷又回来了。这次可以看的细一些，结合张教授的《回到马克思》去重新捋一遍，这次可以重点关注其中的政治经济学内容。</p><h3 id="3-3，恩格斯《反杜林论》"><a href="#3-3，恩格斯《反杜林论》" class="headerlink" title="3.3，恩格斯《反杜林论》"></a>3.3，恩格斯《反杜林论》</h3><p>对于马克思主义你有看不明白的地方，就去看《反杜林论》。</p><p>你和人争辩，觉得对方不对又说不清楚哪里不对的地方，就去看《反杜林论》</p><p>恩格斯yyds！</p><h3 id="3-4-，列宁《唯物主义与经验批判主义》"><a href="#3-4-，列宁《唯物主义与经验批判主义》" class="headerlink" title="3.4 ，列宁《唯物主义与经验批判主义》"></a>3.4 ，列宁《唯物主义与经验批判主义》</h3><p>这是列宁批判唯心主义的大作。也是列宁的哲学思想中最引人争议的地方（指哲学的党性原则）<br>实际上这一篇更重要的地方是对唯物主义认识论的论述。但是关于党性原则的争论似乎是无休止的。</p><p>教员曾经有过论述，自然科学是没有阶级性的。但是对科学的利用和研究是有的。<br>抓住这句话去理解党性原则的问题。</p><h3 id="3-4-，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。"><a href="#3-4-，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。" class="headerlink" title="3.4 ，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。"></a>3.4 ，知网上关于列宁批判民粹主义、列宁批判伯恩施坦主义的综合性论述。</h3><p>这个没必要看原文，虽然列宁的《怎么办》是一篇极为重要的经典著作。</p><p>但是太长了。没必要专门去看。主要是看列宁是如何论证具体的政治经济现状，如何基于他论证的现状去批评不同的思潮的。</p><p>尤其是，列宁为何批评冒进式的民粹主义。如何批评跪在地上挥拳头的工联主义和经济派。<br>这对于当今的舆论环境来说，有特别的意义。</p><p>扩展阅读：知网上关于列宁新经济政策的一系列研读。</p><h2 id="4，书单，第三部分"><a href="#4，书单，第三部分" class="headerlink" title="4，书单，第三部分"></a><strong>4，书单，第三部分</strong></h2><p>我还没看，但觉得有必要看，又或者看了确实没看明白的书目。</p><p>黑格尔《逻辑学》。列宁《哲学笔记》、马克思《黑格尔法哲学批判》，以及张一兵教授对《哲学笔记》的一系列研读</p><p>列宁说过看不懂得逻辑学就不可能懂得辩证唯物主义。而我可以自豪的宣称，我特么真的不懂。</p><p>然后就是对《资本论》、尤其是对政治经济学内容的进一步研读。尤其关注其中概念性和定义性的问题。<br>另外，对于资本论，把握恩格斯的一个论述：资本论并非是宣扬某种理想国或者乌托邦的作品，而是对现实存在的社会制度的剖析。<br>然后到了这里，再去看别的书就可以比较随意了。有需要有兴趣的就找来看</p><h2 id="5，评论区拾遗"><a href="#5，评论区拾遗" class="headerlink" title="5，评论区拾遗"></a><strong>5，评论区拾遗</strong></h2><p>为什么我这个书单里没有近现代中国的著作?<br>一方面是因为我个人的入门方向是列宁主义，对近现代的中国的东西学习尚未能成体系，难以进行有价值的推荐。另一方面，是由于不是很方便讨论。</p><p>但是这里还是补拾了一些评论区里提到的，值得一读的作品。这里再次感谢所有人给出的意见。</p><ol><li>教员的《实践论》《矛盾论》<br>十分重要而且成体系的著作。一开始可以粗读，然后可以放在第二部分再次精读细读。</li><li>恩格斯的《自然辩证法》<br>评论区的知友指出这是适合入门的作品，可以自行取舍。</li><li>徐禾的《政治经济学概论》,李达的《唯物辩证法大纲》<br>国内非常有高度的经典教材。综合性和概念性比较强，有强烈兴趣的朋友可以考虑参考这两本书进行系统性阅读。如果觉得看着吃力，则可以考虑先跳过，遇到问题以后再回来对照着看。</li><li>教员的《中国社会各阶级分析》，《论持久战》，《中国革命战争的战略问题》<br>我估计只要提到了，后面就不用我多说什么了。结合具体的时代背景去了解这几篇。</li><li>恩格斯的《家庭、私有制和国家起源》、列宁的《论国家》<br>作为《哥达纲领批判》《国家与革命》的延伸阅读。另外我个人是十分喜欢恩格斯的文章的。只是实在是最近在划水，从外在需求上来说实在是没能系统性地去看恩格斯的东西。《家庭、私有制和国家起源》作为入门作品而言略有些嫌长了，但是也可以先看一、二、九章。恩格斯在这本书里的论述是唯物史观的集中体现，可以说，唯物史观到底是怎么个东西。读完恩格斯的这一篇，不用别人给你定义，自身也能有个概念了。</li><li>《共产党宣言》<br>有人问《共产党宣言》这么具有代表性的作品为什么不放进来。我在评论区和两位知友探讨了很多。我觉得《宣言》最好是放在后面看。<br>是从马列主义中去理解《宣言》，而不是从《宣言》中去理解马列主义。</li><li>艾思奇的《大众哲学》和《历史唯物主义和辩证唯物主义》<br>国内比较早的经典作品了。不过我没有看过。评论区有知友认为比较适合入门。作为参考</li><li>《政治的逻辑-马克思主义政治学原理》中国近三十年的意识形态领域的问题。</li></ol>]]></content>
    
    
    <categories>
      
      <category>共产党理论相关</category>
      
    </categories>
    
    
    <tags>
      
      <tag>马列主义</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Seq2SeqWithAttention</title>
    <link href="/2022/05/07/Seq2SeqWithAttention/"/>
    <url>/2022/05/07/Seq2SeqWithAttention/</url>
    
    <content type="html"><![CDATA[<h1 id="【转载】Visualizing-A-Neural-Machine-Translation-Model-Mechanics-of-Seq2seq-Models-With-Attention"><a href="#【转载】Visualizing-A-Neural-Machine-Translation-Model-Mechanics-of-Seq2seq-Models-With-Attention" class="headerlink" title="【转载】Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)"></a>【转载】<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/">Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention)</a></h1><p>Sequence-to-sequence models are deep learning models that have achieved a lot of success in tasks like machine translation, text summarization, and image captioning. Google Translate started <a href="https://blog.google/products/translate/found-translation-more-accurate-fluent-sentences-google-translate/">using</a> such a model in production in late 2016. These models are explained in the two pioneering papers (<a href="https://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf">Sutskever et al., 2014</a>, <a href="http://emnlp2014.org/papers/pdf/EMNLP2014179.pdf">Cho et al., 2014</a>).</p><p>I found, however, that understanding the model well enough to implement it requires unraveling a series of concepts that build on top of each other. I thought that a bunch of these ideas would be more accessible if expressed visually. That’s what I aim to do in this post. You’ll need some previous understanding of deep learning to get through this post. I hope it can be a useful companion to reading the papers mentioned above (and the attention papers linked later in the post).</p><p>A sequence-to-sequence model is a model that takes a sequence of items (words, letters, features of an images…etc) and outputs another sequence of items. A trained model would work like this:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_1.gif" width heigh></center><p>In neural machine translation, a sequence is a series of words, processed one after another. The output is, likewise, a series of words:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_2.gif" width heigh></center><h2 id="Looking-under-the-hood"><a href="#Looking-under-the-hood" class="headerlink" title="Looking under the hood"></a>Looking under the hood</h2><p>Under the hood, the model is composed of an encoder and a decoder.</p><p>The encoder processes each item in the input sequence, it compiles the information it captures into a vector (called the context). After processing the entire input sequence, the encoder sends the context over to the decoder, which begins producing the output sequence item by item.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_3.gif" width heigh></center><p>The same applies in the case of machine translation.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_4.gif" width heigh></center><p>The context is a vector (an array of numbers, basically) in the case of machine translation. The encoder and decoder tend to both be recurrent neural networks (Be sure to check out Luis Serrano’s <a href="https://www.youtube.com/watch?v=UNmqTiOnRfg">A friendly introduction to Recurrent Neural Networks</a> for an intro to RNNs).</p><p><img src="/2022/05/07/Seq2SeqWithAttention/context.png" alt="context"></p><p>[^ ]: The context is a vector of floats. Later in this post we will visualize vectors in color by assigning brighter colors to the cells with higher values.</p><p>You can set the size of the context vector when you set up your model. It is basically the number of hidden units in the <font color="red">encoder</font> $ RNN. These visualizations show a vector of size 4, but in real world applications the context vector would be of a size like 256, 512, or 1024.</p><p>By design, a RNN takes two inputs at each time step: an input (in the case of the encoder, one word from the input sentence), and a hidden state. The word, however, needs to be represented by a vector. To transform a word into a vector, we turn to the class of methods called “<a href="https://machinelearningmastery.com/what-are-word-embeddings/">word embedding</a>” algorithms. These turn words into vector spaces that capture a lot of the meaning&#x2F;semantic information of the words (e.g. <a href="http://p.migdal.pl/2017/01/06/king-man-woman-queen-why.html">king - man + woman &#x3D; queen</a>).</p><p><img src="/2022/05/07/Seq2SeqWithAttention/embedding.png"></p><p>[^ ]: We need to turn the input words into vectors before processing them. That transformation is done using a <a href="https://en.wikipedia.org/wiki/Word_embedding">word embedding</a> algorithm. We can use <a href="http://ahogrammer.com/2017/01/20/the-list-of-pretrained-word-embeddings/">pre-trained embeddings</a> or train our own embedding on our dataset. Embedding vectors of size 200 or 300 are typical, we’re showing a vector of size four for simplicity.</p><p>Now that we’ve introduced our main vectors&#x2F;tensors, let’s recap the mechanics of an RNN and establish a visual language to describe these models:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/RNN_1.gif" width heigh></center><p>The next RNN step takes the second input vector and hidden state #1 to create the output of that time step. Later in the post, we’ll use an animation like this to describe the vectors inside a neural machine translation model.</p><p>In the following visualization, each pulse for the encoder or decoder is that RNN processing its inputs and generating an output for that time step. Since the encoder and decoder are both RNNs, each time step one of the RNNs does some processing, it updates its hidden state based on its inputs and previous inputs it has seen.</p><p>Let’s look at the hidden states for the encoder. Notice how the last hidden state is actually the context we pass along to the decoder.<img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_5.gif"></p><p>The decoder also maintains a hidden state that it passes from one time step to the next. We just didn’t visualize it in this graphic because we’re concerned with the major parts of the model for now.</p><p>Let’s now look at another way to visualize a sequence-to-sequence model. This animation will make it easier to understand the static graphics that describe these models. This is called an “unrolled” view where instead of showing the one decoder, we show a copy of it for each time step. This way we can look at the inputs and outputs of each time step.</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_6.gif" width heigh></center><h1 id="Let’s-Pay-Attention-Now"><a href="#Let’s-Pay-Attention-Now" class="headerlink" title="Let’s Pay Attention Now"></a>Let’s Pay Attention Now</h1><p>The context vector turned out to be a bottleneck for these types of models. It made it challenging for the models to deal with long sentences. A solution was proposed in <a href="https://arxiv.org/abs/1409.0473">Bahdanau et al., 2014</a> and <a href="https://arxiv.org/abs/1508.04025">Luong et al., 2015</a>. These papers introduced and refined a technique called “Attention”, which highly improved the quality of machine translation systems. Attention allows the model to focus on the relevant parts of the input sequence as needed.</p><p><img src="/2022/05/07/Seq2SeqWithAttention/attention.png" alt="attention"></p><p>[^ ]: At time step 7, the attention mechanism enables the decoder to focus on the word “étudiant” (“student” in french) before it generates the English translation. This ability to amplify the signal from the relevant part of the input sequence makes attention models produce better results than models without attention.</p><p>Let’s continue looking at attention models at this high level of abstraction. An attention model differs from a classic sequence-to-sequence model in two main ways:</p><p>First, the encoder passes a lot more data to the decoder. Instead of passing the last hidden state of the encoding stage, the encoder passes <em>all</em> the hidden states to the decoder:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_7.gif" width heigh></center><p>Second, an attention decoder does an extra step before producing its output. In order to focus on the parts of the input that are relevant to this decoding time step, the decoder does the following:</p><ol><li>Look at the set of encoder hidden states it received – each encoder hidden state is most associated with a certain word in the input sentence</li><li>Give each hidden state a score (let’s ignore how the scoring is done for now)</li><li>Multiply each hidden state by its softmaxed score, thus amplifying hidden states with high scores, and drowning out hidden states with low scores</li></ol><center><img src="/2022/05/07/Seq2SeqWithAttention/attention_process.gif" width heigh></center><p>This scoring exercise is done at each time step on the decoder side.</p><p>Let us now bring the whole thing together in the following visualization and look at how the attention process works:</p><ol><li>The attention decoder RNN takes in the embedding of the <END> token, and an initial decoder hidden state.</END></li><li>The RNN processes its inputs, producing an output and a new hidden state vector (h4). The output is discarded.</li><li>Attention Step: We use the encoder hidden states and the h4 vector to calculate a context vector (C4) for this time step.</li><li>We concatenate h4 and C4 into one vector.</li><li>We pass this vector through a feedforward neural network (one trained jointly with the model).</li><li>The output of the feedforward neural networks indicates the output word of this time step.</li><li>Repeat for the next time steps</li></ol><center><img src="/2022/05/07/Seq2SeqWithAttention/attention_tensor_dance.gif" width heigh></center><p>This is another way to look at which part of the input sentence we’re paying attention to at each decoding step:</p><center><img src="/2022/05/07/Seq2SeqWithAttention/seq2seq_9.gif"></center><p>Note that the model isn’t just mindless aligning the first word at the output with the first word from the input. It actually learned from the training phase how to align words in that language pair (French and English in our example). An example for how precise this mechanism can be comes from the attention papers listed above:</p><p><img src="/2022/05/07/Seq2SeqWithAttention/attention_sentence.png"></p><p>[^ ]: You can see how the model paid attention correctly when outputing “European Economic Area”. In French, the order of these words is reversed (“européenne économique zone”) as compared to English. Every other word in the sentence is in similar order.</p><p>If you feel you’re ready to learn the implementation, be sure to check TensorFlow’s <a href="https://github.com/tensorflow/nmt">Neural Machine Translation (seq2seq) Tutorial</a>.</p>]]></content>
    
    
    <categories>
      
      <category>注意力机制</category>
      
    </categories>
    
    
    <tags>
      
      <tag>Attention</tag>
      
      <tag>注意力机制</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>My first post</title>
    <link href="/2022/04/28/my-first-post/"/>
    <url>/2022/04/28/my-first-post/</url>
    
    <content type="html"><![CDATA[<p>2022&#x2F;4&#x2F;28 首篇博客。<br>年级大了，怕一些事情一些学过的知识记不住，开始学习用网站记录些什么了。<br>也希望这记录的点点滴滴能督促我更充实地活！</p>]]></content>
    
    
    <categories>
      
      <category>生活</category>
      
    </categories>
    
    
    <tags>
      
      <tag>练习</tag>
      
      <tag>开篇</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Hello World</title>
    <link href="/2022/04/28/hello-world/"/>
    <url>/2022/04/28/hello-world/</url>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo new <span class="hljs-string">&quot;My New Post&quot;</span><br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo server<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo generate<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">$ hexo deploy<br></code></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
    
  </entry>
  
  
  
  
</search>
